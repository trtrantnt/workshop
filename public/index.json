[
{
	"uri": "//localhost:62814/1-gioi-thieu/",
	"title": "1. Introduction",
	"tags": [],
	"description": "",
	"content": "Workshop Overview This workshop will guide you through implementing a comprehensive Identity Governance system with Access Certification on AWS, including:\nAccess Governance: Managing and controlling access rights Certification Automation: Automating access certification processes Privilege Analytics: Analyzing and monitoring privileges Risk Assessment: Security risk evaluation Monitoring Setup: Setting up continuous monitoring Operational Procedures: Operational processes Audit Procedures: Audit processes Compliance Validation: Compliance verification Overall Architecture AWS Services Used (Minimal Architecture) AWS IAM Identity Center - Centralized access management AWS IAM - Identity and access management AWS Lambda - Automation and processing functions Amazon EventBridge - Event-driven orchestration Amazon DynamoDB - Data storage for certifications Amazon S3 - Log storage and data lake AWS CloudTrail - Audit logging Amazon CloudWatch - Monitoring and metrics Amazon SNS - Notifications and alerts Amazon QuickSight - Analytics dashboard AWS Security Hub - Risk assessment and compliance Benefits of Identity Governance 1. Enhanced Security Strict access control Security risk detection and prevention Continuous activity monitoring 2. Regulatory Compliance Meet SOX, SOC2, ISO27001 requirements Automated audit processes Compliance evidence storage 3. Operational Efficiency Automated certification processes Reduced manual work Improved management processes Completion Time Approximately 4-6 hours (can be divided into multiple sessions)\nNext Steps Continue to 2. Preparation Steps to start setting up the environment.\n"
},
{
	"uri": "//localhost:62814/",
	"title": "Identity Governance with Access Certification",
	"tags": [],
	"description": "",
	"content": "Identity Governance with Access Certification Workshop Workshop Overview This comprehensive workshop guides you through implementing Identity Governance with Access Certification on AWS, covering advanced security practices and compliance requirements.\nWorkshop Modules 1. Introduction\r2. Preparation Steps\r3. Access Governance Setup\r4. Certification Automation\r5. Privilege Analytics\r6. Risk Assessment\r7. Monitoring Setup\r8. Operational Procedures\r9. Audit Procedures\r10. Compliance Validation\r11. Clean Resources\rArchitecture Overview Key Benefits Enhanced Security: Strict access control and continuous monitoring Regulatory Compliance: Meet SOX, SOC2, ISO27001 requirements Operational Efficiency: Automated certification and remediation processes Risk Management: Proactive risk assessment and mitigation Prerequisites AWS Account with Administrator privileges Basic understanding of AWS IAM and Organizations Knowledge of compliance frameworks Python and AWS CLI experience "
},
{
	"uri": "//localhost:62814/2-cac-buoc-chuan-bi/",
	"title": "2. Preparation Steps",
	"tags": [],
	"description": "",
	"content": "Environment Setup 1. Create S3 Buckets for Data Storage Navigate to Amazon S3 service in the AWS Console Click Create bucket Create first bucket for analytics data: Bucket name: identity-governance-analytics AWS Region: Select your preferred region (e.g., us-east-1) Object Ownership: ACLs disabled (recommended) Block Public Access settings: Keep all blocked (recommended) Bucket Versioning: Enable Default encryption: Server-side encryption with Amazon S3 managed keys (SSE-S3) Bucket Key: Enable Click Create bucket Create second bucket for compliance reports: Bucket name: identity-governance-reports AWS Region: Same as first bucket Object Ownership: ACLs disabled (recommended) Block Public Access settings: Keep all blocked (recommended) Bucket Versioning: Enable Default encryption: Server-side encryption with Amazon S3 managed keys (SSE-S3) Bucket Key: Enable Object Lock: Enable for compliance retention Click Create bucket Verify both buckets are created successfully: Infrastructure Preparation 1. Enable AWS CloudTrail Navigate to CloudTrail service in AWS Console Click Create trail Step 1: General details Enter basic information: Trail name: IdentityGovernanceTrail Enable for all accounts in my organization: Leave unchecked Step 2: S3 bucket configuration Configure S3 storage:\nCreate new S3 bucket: Select this option (LEAVE BLANK - DO NOT select \u0026ldquo;Use existing S3 bucket\u0026rdquo;) S3 bucket name: CloudTrail will auto-generate name (e.g., aws-cloudtrail-logs-123456789012-abc12345) Configure security settings:\nLog file SSE-KMS encryption: Unchecked (keep default) Log file validation: Checked (recommended) Step 3: CloudWatch Logs (Optional) CloudWatch Logs configuration:\nCloudWatch Logs: Unchecked (skip for now) Click Next\nStep 4: Choose log events Select event types to log: Management events: Checked Read: Checked Write: Checked Data events: Unchecked (skip) Insight events: Unchecked (skip) Click Next Step 5: Review and create Review configuration: Confirm trail name Confirm new S3 bucket will be created Confirm management events are enabled Click Create trail IMPORTANT: NEVER select the identity-governance-analytics bucket or any bucket you created before ALWAYS choose \u0026ldquo;Create new S3 bucket\u0026rdquo; to let CloudTrail create its own bucket CloudTrail will automatically configure the correct bucket policy, avoiding InsufficientS3BucketPolicyException error 2. Enable AWS Security Hub Navigate to AWS Security Hub service in AWS Console You\u0026rsquo;ll see the Security Hub Onboard page Step 1: Configure Security Hub In the Configure Security Hub section: Read information about Service Linked Roles (SLRs) Keep default settings Step 2: Delegated Administrator Account In the Delegated administrator account section: Choose Do not select an account (for single account setup) Step 3: Account Enablement In the Account enablement section: ☑️ Enable Security Hub for my account (keep checked) Step 4: Delegated Administrator Policy In the Delegated administrator policy section: Read policy details Keep default settings Click Onboard at the bottom of the page Step 5: Verify successful activation After successful onboarding, you\u0026rsquo;ll see the Security Hub dashboard: Security score displayed Findings start being collected Standards automatically enabled 3. Create DynamoDB Tables Navigate to DynamoDB service Click Create table Create first table: Table name: AccessCertifications Partition key: UserId (String) Sort key: CertificationDate (String) Billing mode: On-demand Click Create table\nCreate second table:\nTable name: RiskAssessments Partition key: AssessmentId (String) Billing mode: On-demand Click Create table\n4. Create Required IAM Roles Navigate to IAM service Click Roles in the sidebar Click Create role Create role for Lambda: Trusted entity: AWS service Service: Lambda Role name: IdentityGovernanceLambdaRole Policies: Attach AWSLambdaBasicExecutionRole Verification 1. Check Enabled Services CloudTrail: Go to CloudTrail console, confirm trail is created and active S3: Go to S3 console, confirm 3 buckets created (2 your buckets + 1 CloudTrail bucket) Security Hub: Go to Security Hub console, confirm service is enabled with security score DynamoDB: Go to DynamoDB console, confirm 2 tables created IAM: Go to IAM console, confirm Lambda role is created 2. Check Access Permissions Go to IAM console Click Users and confirm current user has required permissions Click Roles and confirm roles are created Expected Results After completing the preparation steps:\n✅ AWS Account properly configured ✅ Required AWS services enabled ✅ Base infrastructure deployed ✅ Permissions validated ✅ Workshop materials ready Next Steps Continue to 3. Access Governance Setup to start implementing the system.\n"
},
{
	"uri": "//localhost:62814/3-thiet-lap-access-governance/",
	"title": "3. Access Governance Setup",
	"tags": [],
	"description": "",
	"content": "Objective Set up centralized access management foundation with AWS IAM Identity Center and IAM.\nStep 1: IAM Foundation Setup 1.1 Create IAM Groups Navigate to IAM service in AWS Console Click User groups in the sidebar Click Create group Enter group information: Group name: SecurityAuditors Click Create group 1.2 Create IAM Policies Click Policies in the sidebar Click Create policy Use JSON editor to create custom policy: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:Get*\u0026#34;, \u0026#34;iam:List*\u0026#34;, \u0026#34;iam:Generate*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudtrail:Get*\u0026#34;, \u0026#34;cloudtrail:List*\u0026#34;, \u0026#34;cloudtrail:Describe*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Name the policy: SecurityAuditPolicy Click Create policy Step 2: Configure IAM Identity Center 2.1 Enable IAM Identity Center Search and open IAM Identity Center in AWS Console Click Enable to activate IAM Identity Center Choose region to store identity store Select Use IAM Identity Center as my identity source 2.2 Create Permission Sets In IAM Identity Center, click Permission sets in the sidebar Click Create permission set Select Predefined permission set Choose SecurityAudit from dropdown Enter information: Name: SecurityAuditor Description: Read-only access for security auditing Session duration: 8 hours Click Next and Create Step 3: Identity Store Setup 3.1 Create Users and Groups In IAM Identity Center, click Users in the sidebar\nClick Add user\nEnter user information:\nUsername: security-auditor Email: auditor@company.com First name: Security Last name: Auditor Click Next and Add user\n3.2 Create Groups Click Groups in the sidebar\nClick Create group\nEnter:\nGroup name: SecurityAuditors Description: Security auditing team Add users to group\nSelect user security-auditor Click Create group\nStep 4: Assign Access 4.1 Assign Permission Sets to Accounts Click AWS accounts in the sidebar\nSelect the account to assign permissions\nClick Assign users or groups\nSelect Groups tab\nSelect group SecurityAuditors\nClick Next\nSelect permission set SecurityAuditor\nClick Next and Submit\nExpected Results After completing this step, you will have:\n✅ IAM Groups and Policies configured ✅ IAM Identity Center activated ✅ Permission Sets for governance roles ✅ Identity Store with groups and users ✅ Access assignments configured Next Steps Continue to 4. Certification Automation to set up automated certification processes.\n"
},
{
	"uri": "//localhost:62814/4-tu-dong-hoa-certification/",
	"title": "4. Certification Automation",
	"tags": [],
	"description": "",
	"content": "Objective Automate access certification processes to ensure access rights are reviewed periodically and comply with security requirements.\nStep 1: Verify DynamoDB Table 1.1 Check Existing Table Open Amazon DynamoDB in the console Verify that the AccessCertifications table was created in chapter 2 This table will be used to store certification data Step 2: Create Lambda Function 2.1 Create Lambda Function Open AWS Lambda in the console Click Create function Choose Author from scratch Enter function details: Function name: AccessCertificationTrigger Runtime: Python 3.9 Architecture: x86_64 Click Create function 2.2 Configure Lambda Function Code In the Code tab, replace the default code with the following: import json import boto3 from datetime import datetime def lambda_handler(event, context): print(\u0026#34;Access Certification Trigger Started\u0026#34;) # Initialize AWS clients dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamodb.Table(\u0026#39;AccessCertifications\u0026#39;) # Create certification record response = table.put_item( Item={ \u0026#39;UserId\u0026#39;: \u0026#39;system\u0026#39;, \u0026#39;CertificationDate\u0026#39;: datetime.now().isoformat(), \u0026#39;Status\u0026#39;: \u0026#39;Triggered\u0026#39;, \u0026#39;Type\u0026#39;: \u0026#39;Quarterly Review\u0026#39; } ) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Certification process triggered successfully\u0026#39;) } Click Deploy to save changes 2.3 Configure IAM Role for Lambda Go to Configuration tab Click Permissions Click on the role name to open IAM console Click Add permissions → Attach policies Search and attach policy AmazonDynamoDBFullAccess Select Add permissions Step 3: Setup EventBridge Scheduler 3.1 Create Scheduled Rule Open Amazon EventBridge in AWS Console Click Rules in the sidebar Click Create rule Step 1: Define rule detail Enter rule information:\nName: AccessCertificationSchedule Description: Quarterly access certification review Event bus: default Enable the rule on the selected event bus In Rule type, select Schedule\nClick Next\nStep 2: Define schedule In Occurrence, select Recurring schedule In Schedule pattern, select Rate-based schedule Enter 90 and select Days In Flexible time window, enter 15 minutes Click Next Step 3: Select target In Target API, select AWS Lambda Invoke In Lambda function, select AccessCertificationTrigger Click Next Step 4: Configure tags (Optional) Skip the tags section, click Next Step 5: Review and create Review configuration and click Create rule Step 4: Test the Automation 4.1 Verify EventBridge Schedule In Amazon EventBridge console Click Schedules in the sidebar (not Rules) Verify that schedule AccessCertificationSchedule is created and Enabled 4.2 Test Lambda Function Manually Go to AWS Lambda console Select function AccessCertificationTrigger Click Test to create a test event Use default test event and click Test Check execution results 4.3 Verify DynamoDB Record Go to Amazon DynamoDB console Select table AccessCertifications Click Explore table items Verify that a new record was created by the Lambda function Expected Results After completion:\n✅ DynamoDB table for certification data storage ✅ Lambda function processing certification logic ✅ EventBridge scheduled triggers quarterly ✅ Automated quarterly access reviews ✅ Audit trail and monitoring Next Steps Continue to 5. Privilege Analytics to set up privilege analysis.\n"
},
{
	"uri": "//localhost:62814/5-phan-tich-dac-quyen/",
	"title": "5. Privilege Analytics",
	"tags": [],
	"description": "",
	"content": "Objective Analyze and monitor privilege usage to detect security risks, excessive permissions, and abnormal patterns through CloudTrail logs.\nStep 1: Verify CloudTrail Data 1.1 Check CloudTrail Logs Open Amazon CloudTrail in the console Verify that trail IdentityGovernanceTrail was created in chapter 2 Check S3 bucket containing CloudTrail logs 1.2 Verify S3 Bucket has CloudTrail Data Go to Amazon S3 console Find CloudTrail bucket (name like aws-cloudtrail-logs-xxx) Verify that log files are being created Step 2: Create Lambda Function for Privilege Analytics 2.1 Create Lambda Function Open AWS Lambda in the console Click Create function Choose Author from scratch Enter function details: Function name: PrivilegeAnalyticsEngine Runtime: Python 3.9 Architecture: x86_64 Click Create function 2.2 Configure Lambda Function Code In the Code tab, replace the default code with the following: import json import boto3 import gzip from datetime import datetime, timedelta from urllib.parse import unquote_plus def lambda_handler(event, context): print(\u0026#34;Privilege Analytics Engine Started\u0026#34;) # Initialize AWS clients s3 = boto3.client(\u0026#39;s3\u0026#39;) dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) # Get the object from the event bucket = event[\u0026#39;Records\u0026#39;][0][\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] key = unquote_plus(event[\u0026#39;Records\u0026#39;][0][\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;]) try: # Download and decompress CloudTrail log response = s3.get_object(Bucket=bucket, Key=key) if key.endswith(\u0026#39;.gz\u0026#39;): content = gzip.decompress(response[\u0026#39;Body\u0026#39;].read()) else: content = response[\u0026#39;Body\u0026#39;].read() # Parse CloudTrail log log_data = json.loads(content.decode(\u0026#39;utf-8\u0026#39;)) # Analyze privilege usage privilege_events = analyze_privilege_events(log_data[\u0026#39;Records\u0026#39;]) # Store analysis results store_analysis_results(privilege_events, dynamodb) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(f\u0026#39;Processed {len(privilege_events)} privilege events\u0026#39;) } except Exception as e: print(f\u0026#39;Error processing {key}: {str(e)}\u0026#39;) return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: json.dumps(f\u0026#39;Error: {str(e)}\u0026#39;) } def analyze_privilege_events(records): \u0026#34;\u0026#34;\u0026#34;Analyze CloudTrail records for privilege usage patterns\u0026#34;\u0026#34;\u0026#34; privilege_events = [] high_privilege_actions = [ \u0026#39;CreateUser\u0026#39;, \u0026#39;DeleteUser\u0026#39;, \u0026#39;AttachUserPolicy\u0026#39;, \u0026#39;DetachUserPolicy\u0026#39;, \u0026#39;CreateRole\u0026#39;, \u0026#39;DeleteRole\u0026#39;, \u0026#39;AttachRolePolicy\u0026#39;, \u0026#39;DetachRolePolicy\u0026#39;, \u0026#39;PutUserPolicy\u0026#39;, \u0026#39;DeleteUserPolicy\u0026#39;, \u0026#39;PutRolePolicy\u0026#39;, \u0026#39;DeleteRolePolicy\u0026#39; ] for record in records: event_name = record.get(\u0026#39;eventName\u0026#39;, \u0026#39;\u0026#39;) if event_name in high_privilege_actions: privilege_event = { \u0026#39;eventTime\u0026#39;: record.get(\u0026#39;eventTime\u0026#39;), \u0026#39;eventName\u0026#39;: event_name, \u0026#39;userIdentity\u0026#39;: record.get(\u0026#39;userIdentity\u0026#39;, {}), \u0026#39;sourceIPAddress\u0026#39;: record.get(\u0026#39;sourceIPAddress\u0026#39;), \u0026#39;userAgent\u0026#39;: record.get(\u0026#39;userAgent\u0026#39;), \u0026#39;awsRegion\u0026#39;: record.get(\u0026#39;awsRegion\u0026#39;), \u0026#39;riskScore\u0026#39;: calculate_risk_score(record) } privilege_events.append(privilege_event) return privilege_events def calculate_risk_score(record): \u0026#34;\u0026#34;\u0026#34;Calculate risk score for privilege event (1-10 scale)\u0026#34;\u0026#34;\u0026#34; base_score = 5 # High-risk actions high_risk_actions = [\u0026#39;DeleteUser\u0026#39;, \u0026#39;DeleteRole\u0026#39;, \u0026#39;DetachUserPolicy\u0026#39;] if record.get(\u0026#39;eventName\u0026#39;) in high_risk_actions: base_score += 3 # External IP access source_ip = record.get(\u0026#39;sourceIPAddress\u0026#39;, \u0026#39;\u0026#39;) if not source_ip.startswith(\u0026#39;10.\u0026#39;) and not source_ip.startswith(\u0026#39;172.\u0026#39;) and not source_ip.startswith(\u0026#39;192.168.\u0026#39;): base_score += 2 # Console vs API access user_agent = record.get(\u0026#39;userAgent\u0026#39;, \u0026#39;\u0026#39;) if \u0026#39;console\u0026#39; not in user_agent.lower(): base_score += 1 return min(base_score, 10) def store_analysis_results(privilege_events, dynamodb): \u0026#34;\u0026#34;\u0026#34;Store analysis results in DynamoDB\u0026#34;\u0026#34;\u0026#34; table = dynamodb.Table(\u0026#39;RiskAssessments\u0026#39;) for event in privilege_events: table.put_item( Item={ \u0026#39;AssessmentId\u0026#39;: f\u0026#34;privilege-{datetime.now().isoformat()}\u0026#34;, \u0026#39;EventTime\u0026#39;: event[\u0026#39;eventTime\u0026#39;], \u0026#39;EventName\u0026#39;: event[\u0026#39;eventName\u0026#39;], \u0026#39;UserIdentity\u0026#39;: json.dumps(event[\u0026#39;userIdentity\u0026#39;]), \u0026#39;SourceIP\u0026#39;: event[\u0026#39;sourceIPAddress\u0026#39;], \u0026#39;RiskScore\u0026#39;: event[\u0026#39;riskScore\u0026#39;], \u0026#39;AssessmentType\u0026#39;: \u0026#39;Privilege Analysis\u0026#39; } ) Click Deploy to save changes 2.3 Configure IAM Role for Lambda Go to Configuration tab Click Permissions Click on the role name to open IAM console Click Add permissions → Attach policies Search and attach the following policies: AmazonS3ReadOnlyAccess AmazonDynamoDBFullAccess Click Add permissions Step 3: Setup S3 Event Trigger 3.1 Configure S3 Trigger for Lambda In Lambda function PrivilegeAnalyticsEngine Click Add trigger Select S3 from dropdown Configure trigger: Bucket: Select CloudTrail S3 bucket Event type: All object create events Prefix: AWSLogs/ (optional) Suffix: .json.gz Click Add Step 4: Create CloudWatch Dashboard 4.1 Create Dashboard for Privilege Analytics Open Amazon CloudWatch console Click Dashboards in the sidebar Click Create dashboard Enter dashboard name: PrivilegeAnalyticsDashboard Click Create dashboard 4.2 Add Widget 1: Lambda Invocations Click Add widget Select Line chart Click Next Configure metric: Namespace: AWS/Lambda Metric name: Invocations Dimensions: FunctionName = PrivilegeAnalyticsEngine Click Select metric Set widget name: \u0026ldquo;Lambda Invocations\u0026rdquo; Click Create widget 4.3 Add Widget 2: Lambda Errors Click Add widget (in dashboard) Select Line chart → Next Configure metric: Namespace: AWS/Lambda Metric name: Errors Dimensions: FunctionName = PrivilegeAnalyticsEngine Click Select metric Set widget name: \u0026ldquo;Lambda Errors\u0026rdquo; Click Create widget 4.4 Add Widget 3: Lambda Duration Click Add widget Select Line chart → Next Configure metric: Namespace: AWS/Lambda Metric name: Duration Dimensions: FunctionName = PrivilegeAnalyticsEngine Click Select metric Set widget name: \u0026ldquo;Lambda Duration (ms)\u0026rdquo; Click Create widget 4.5 Add Widget 4: DynamoDB Write Activity Click Add widget Select Line chart → Next Configure metric: Namespace: AWS/DynamoDB Metric name: ConsumedWriteCapacityUnits Dimensions: TableName = RiskAssessments Click Select metric Set widget name: \u0026ldquo;DynamoDB Write Activity\u0026rdquo; Click Create widget Note: This metric shows write activity to DynamoDB, indicating when new risk assessments are stored.\n4.6 Save Dashboard Click Save dashboard in the top right corner Dashboard will display 4 widgets monitoring: Number of Lambda invocations Lambda errors Lambda execution time Number of items in DynamoDB Step 5: Test Privilege Analytics 5.1 Create Test User to Generate CloudTrail Events Go to IAM console Click Users in the left sidebar Click Create user Enter User name: test-privilege-user Click Next Select Attach policies directly Search and select ReadOnlyAccess Click Next → Create user 5.2 Perform High-Privilege Actions In IAM console, select the test-privilege-user you just created Click Permissions tab Click Add permissions → Attach policies directly Search and attach PowerUserAccess policy Click Add permissions Then Remove the PowerUserAccess policy to create more events Create test role: Click Roles in sidebar Click Create role Select AWS service → Lambda Click Next → Next Role name: test-privilege-role Click Create role 5.3 Wait for CloudTrail Processing CloudTrail needs time to write logs to S3 Check CloudTrail S3 bucket: Go to S3 console Find CloudTrail bucket (name like aws-cloudtrail-logs-xxx) Verify new log files are being created 5.4 Verify Lambda Function Execution Go to AWS Lambda console Select function PrivilegeAnalyticsEngine Click Monitor tab Check Invocations graph - should show activity Click View CloudWatch logs Select the latest log stream Verify logs like: Privilege Analytics Engine Started Processed X privilege events 5.5 Verify DynamoDB Records Go to Amazon DynamoDB console Click Tables in sidebar Select table RiskAssessments Click Explore table items Verify new records with: AssessmentType: \u0026lsquo;Privilege Analysis\u0026rsquo; EventName: \u0026lsquo;AttachUserPolicy\u0026rsquo;, \u0026lsquo;DetachUserPolicy\u0026rsquo;, \u0026lsquo;CreateRole\u0026rsquo; RiskScore: Value from 1-10 EventTime: Recent timestamp 5.6 Check CloudWatch Dashboard Go to CloudWatch console Click Dashboards Select PrivilegeAnalyticsDashboard Verify widgets display data: Lambda Invocations: Should show spike when function runs Lambda Errors: Should be 0 Lambda Duration: Execution time DynamoDB Write Activity: Should show activity when writing data 5.7 Test Real-time Monitoring Perform additional privilege action: Create new user: test-user-2 Attach policy IAMReadOnlyAccess Wait 5-10 minutes Refresh DynamoDB table to see new record Check if dashboard updates metrics 5.8 Troubleshooting (if no data) If Lambda doesn\u0026rsquo;t run:\nCheck S3 trigger is configured correctly Verify CloudTrail is creating log files in S3 Check IAM permissions of Lambda role If no data in DynamoDB:\nCheck CloudWatch logs of Lambda function Verify table name in code: \u0026lsquo;RiskAssessments\u0026rsquo; Confirm Lambda has write permissions to DynamoDB If Dashboard doesn\u0026rsquo;t show data:\nWait 5-15 minutes for metrics to appear Check metric names and dimensions are correct Refresh dashboard page Expected Results After completion:\n✅ CloudTrail logs are automatically analyzed ✅ Lambda function processes privilege events ✅ Risk scoring for privilege actions ✅ DynamoDB stores analysis results ✅ CloudWatch dashboard monitoring ✅ Real-time privilege monitoring Next Steps Proceed to 6. Risk Assessment to set up comprehensive risk assessment.\n"
},
{
	"uri": "//localhost:62814/6-danh-gia-rui-ro/",
	"title": "6. Risk Assessment",
	"tags": [],
	"description": "",
	"content": "Objective Establish a comprehensive risk assessment system to analyze and score security risks based on data from previous chapters.\nStep 1: Verify Data from Previous Chapters 1.1 Check DynamoDB Tables Open Amazon DynamoDB in the console Verify tables have data: AccessCertifications (from chapter 4) RiskAssessments (from chapter 5) 1.2 Check Security Hub Findings Go to AWS Security Hub console Verify findings are being collected Check active compliance standards Step 2: Create Lambda Function for Risk Assessment 2.1 Create Lambda Function Open AWS Lambda in the console\nClick Create function\nChoose Author from scratch\nEnter function details:\nFunction name: RiskAssessmentEngine Runtime: Python 3.9 Architecture: x86_64 Click Create function\n2.2 Configure Lambda Function Code In the Code tab, replace the default code with the following: import json import boto3 from datetime import datetime, timedelta from decimal import Decimal def lambda_handler(event, context): print(\u0026#34;Risk Assessment Engine Started\u0026#34;) # Initialize AWS clients dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) securityhub = boto3.client(\u0026#39;securityhub\u0026#39;) iam = boto3.client(\u0026#39;iam\u0026#39;) try: # Collect data from various sources certification_data = get_certification_data(dynamodb) privilege_data = get_privilege_analysis_data(dynamodb) security_findings = get_security_hub_findings(securityhub) iam_data = get_iam_data(iam) # Perform comprehensive risk assessment risk_assessment = perform_risk_assessment( certification_data, privilege_data, security_findings, iam_data ) # Store assessment results store_risk_assessment(risk_assessment, dynamodb) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;message\u0026#39;: \u0026#39;Risk assessment completed\u0026#39;, \u0026#39;totalUsers\u0026#39;: len(risk_assessment[\u0026#39;user_risks\u0026#39;]), \u0026#39;highRiskUsers\u0026#39;: len([u for u in risk_assessment[\u0026#39;user_risks\u0026#39;] if u[\u0026#39;riskLevel\u0026#39;] == \u0026#39;HIGH\u0026#39;]), \u0026#39;overallRiskScore\u0026#39;: risk_assessment[\u0026#39;overall_risk_score\u0026#39;] }) } except Exception as e: print(f\u0026#39;Error in risk assessment: {str(e)}\u0026#39;) return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: json.dumps(f\u0026#39;Error: {str(e)}\u0026#39;) } def get_certification_data(dynamodb): \u0026#34;\u0026#34;\u0026#34;Get certification data from DynamoDB\u0026#34;\u0026#34;\u0026#34; table = dynamodb.Table(\u0026#39;AccessCertifications\u0026#39;) response = table.scan() return response[\u0026#39;Items\u0026#39;] def get_privilege_analysis_data(dynamodb): \u0026#34;\u0026#34;\u0026#34;Get privilege analysis data from DynamoDB\u0026#34;\u0026#34;\u0026#34; table = dynamodb.Table(\u0026#39;RiskAssessments\u0026#39;) response = table.scan( FilterExpression=\u0026#39;AssessmentType = :type\u0026#39;, ExpressionAttributeValues={\u0026#39;:type\u0026#39;: \u0026#39;Privilege Analysis\u0026#39;} ) return response[\u0026#39;Items\u0026#39;] def get_security_hub_findings(securityhub): \u0026#34;\u0026#34;\u0026#34;Get Security Hub findings\u0026#34;\u0026#34;\u0026#34; try: response = securityhub.get_findings( Filters={ \u0026#39;RecordState\u0026#39;: [{\u0026#39;Value\u0026#39;: \u0026#39;ACTIVE\u0026#39;, \u0026#39;Comparison\u0026#39;: \u0026#39;EQUALS\u0026#39;}], \u0026#39;WorkflowStatus\u0026#39;: [{\u0026#39;Value\u0026#39;: \u0026#39;NEW\u0026#39;, \u0026#39;Comparison\u0026#39;: \u0026#39;EQUALS\u0026#39;}] }, MaxResults=100 ) return response[\u0026#39;Findings\u0026#39;] except Exception as e: print(f\u0026#39;Error getting Security Hub findings: {str(e)}\u0026#39;) return [] def get_iam_data(iam): \u0026#34;\u0026#34;\u0026#34;Get IAM users and roles data\u0026#34;\u0026#34;\u0026#34; users = [] roles = [] try: # Get users paginator = iam.get_paginator(\u0026#39;list_users\u0026#39;) for page in paginator.paginate(): users.extend(page[\u0026#39;Users\u0026#39;]) # Get roles paginator = iam.get_paginator(\u0026#39;list_roles\u0026#39;) for page in paginator.paginate(): roles.extend(page[\u0026#39;Roles\u0026#39;]) except Exception as e: print(f\u0026#39;Error getting IAM data: {str(e)}\u0026#39;) return {\u0026#39;users\u0026#39;: users, \u0026#39;roles\u0026#39;: roles} def perform_risk_assessment(cert_data, priv_data, sec_findings, iam_data): \u0026#34;\u0026#34;\u0026#34;Perform comprehensive risk assessment\u0026#34;\u0026#34;\u0026#34; user_risks = [] # Assess each user for user in iam_data[\u0026#39;users\u0026#39;]: user_name = user[\u0026#39;UserName\u0026#39;] # Calculate risk factors cert_risk = calculate_certification_risk(user_name, cert_data) priv_risk = calculate_privilege_risk(user_name, priv_data) security_risk = calculate_security_findings_risk(user_name, sec_findings) access_risk = calculate_access_pattern_risk(user) # Calculate overall user risk score overall_score = (cert_risk * 0.25 + priv_risk * 0.35 + security_risk * 0.25 + access_risk * 0.15) risk_level = get_risk_level(overall_score) user_risk = { \u0026#39;userName\u0026#39;: user_name, \u0026#39;riskScore\u0026#39;: round(overall_score, 2), \u0026#39;riskLevel\u0026#39;: risk_level, \u0026#39;factors\u0026#39;: { \u0026#39;certification\u0026#39;: cert_risk, \u0026#39;privilege\u0026#39;: priv_risk, \u0026#39;security\u0026#39;: security_risk, \u0026#39;access\u0026#39;: access_risk }, \u0026#39;assessmentDate\u0026#39;: datetime.now().isoformat() } user_risks.append(user_risk) # Calculate overall organizational risk if user_risks: overall_risk_score = sum(u[\u0026#39;riskScore\u0026#39;] for u in user_risks) / len(user_risks) else: overall_risk_score = 0 return { \u0026#39;user_risks\u0026#39;: user_risks, \u0026#39;overall_risk_score\u0026#39;: round(overall_risk_score, 2), \u0026#39;assessment_date\u0026#39;: datetime.now().isoformat(), \u0026#39;total_findings\u0026#39;: len(sec_findings) } def calculate_certification_risk(user_name, cert_data): \u0026#34;\u0026#34;\u0026#34;Calculate certification-related risk\u0026#34;\u0026#34;\u0026#34; base_risk = 5.0 # Check if user has recent certifications user_certs = [c for c in cert_data if c.get(\u0026#39;UserId\u0026#39;) == user_name] if not user_certs: return 8.0 # High risk if no certifications # Check certification recency recent_cert = max(user_certs, key=lambda x: x.get(\u0026#39;CertificationDate\u0026#39;, \u0026#39;\u0026#39;)) cert_date = datetime.fromisoformat(recent_cert[\u0026#39;CertificationDate\u0026#39;].replace(\u0026#39;Z\u0026#39;, \u0026#39;+00:00\u0026#39;)) days_since_cert = (datetime.now(cert_date.tzinfo) - cert_date).days if days_since_cert \u0026gt; 90: base_risk += 2 elif days_since_cert \u0026gt; 30: base_risk += 1 return min(base_risk, 10.0) def calculate_privilege_risk(user_name, priv_data): \u0026#34;\u0026#34;\u0026#34;Calculate privilege-related risk\u0026#34;\u0026#34;\u0026#34; user_priv_events = [p for p in priv_data if user_name in str(p.get(\u0026#39;UserIdentity\u0026#39;, \u0026#39;\u0026#39;))] if not user_priv_events: return 3.0 # Low risk if no privilege events # Calculate average risk score from privilege events avg_risk = sum(float(p.get(\u0026#39;RiskScore\u0026#39;, 5)) for p in user_priv_events) / len(user_priv_events) return min(avg_risk, 10.0) def calculate_security_findings_risk(user_name, findings): \u0026#34;\u0026#34;\u0026#34;Calculate risk based on Security Hub findings\u0026#34;\u0026#34;\u0026#34; base_risk = 2.0 # Count findings related to IAM/access iam_findings = [f for f in findings if \u0026#39;iam\u0026#39; in f.get(\u0026#39;Type\u0026#39;, \u0026#39;\u0026#39;).lower() or \u0026#39;access\u0026#39; in f.get(\u0026#39;Title\u0026#39;, \u0026#39;\u0026#39;).lower()] if len(iam_findings) \u0026gt; 5: base_risk += 4 elif len(iam_findings) \u0026gt; 2: base_risk += 2 elif len(iam_findings) \u0026gt; 0: base_risk += 1 return min(base_risk, 10.0) def calculate_access_pattern_risk(user): \u0026#34;\u0026#34;\u0026#34;Calculate risk based on access patterns\u0026#34;\u0026#34;\u0026#34; base_risk = 3.0 # Check password age password_last_used = user.get(\u0026#39;PasswordLastUsed\u0026#39;) if password_last_used: days_since_login = (datetime.now(password_last_used.tzinfo) - password_last_used).days if days_since_login \u0026gt; 90: base_risk += 2 elif days_since_login \u0026gt; 30: base_risk += 1 else: base_risk += 3 # Never logged in return min(base_risk, 10.0) def get_risk_level(score): \u0026#34;\u0026#34;\u0026#34;Convert risk score to risk level\u0026#34;\u0026#34;\u0026#34; if score \u0026gt;= 8: return \u0026#39;CRITICAL\u0026#39; elif score \u0026gt;= 6: return \u0026#39;HIGH\u0026#39; elif score \u0026gt;= 4: return \u0026#39;MEDIUM\u0026#39; else: return \u0026#39;LOW\u0026#39; def store_risk_assessment(assessment, dynamodb): \u0026#34;\u0026#34;\u0026#34;Store risk assessment results in DynamoDB\u0026#34;\u0026#34;\u0026#34; table = dynamodb.Table(\u0026#39;RiskAssessments\u0026#39;) # Store overall assessment table.put_item( Item={ \u0026#39;AssessmentId\u0026#39;: f\u0026#34;risk-assessment-{datetime.now().isoformat()}\u0026#34;, \u0026#39;AssessmentType\u0026#39;: \u0026#39;Comprehensive Risk Assessment\u0026#39;, \u0026#39;OverallRiskScore\u0026#39;: Decimal(str(assessment[\u0026#39;overall_risk_score\u0026#39;])), \u0026#39;TotalUsers\u0026#39;: len(assessment[\u0026#39;user_risks\u0026#39;]), \u0026#39;HighRiskUsers\u0026#39;: len([u for u in assessment[\u0026#39;user_risks\u0026#39;] if u[\u0026#39;riskLevel\u0026#39;] in [\u0026#39;HIGH\u0026#39;, \u0026#39;CRITICAL\u0026#39;]]), \u0026#39;AssessmentDate\u0026#39;: assessment[\u0026#39;assessment_date\u0026#39;] } ) # Store individual user risks for user_risk in assessment[\u0026#39;user_risks\u0026#39;]: table.put_item( Item={ \u0026#39;AssessmentId\u0026#39;: f\u0026#34;user-risk-{user_risk[\u0026#39;userName\u0026#39;]}-{datetime.now().isoformat()}\u0026#34;, \u0026#39;AssessmentType\u0026#39;: \u0026#39;User Risk Assessment\u0026#39;, \u0026#39;UserName\u0026#39;: user_risk[\u0026#39;userName\u0026#39;], \u0026#39;RiskScore\u0026#39;: Decimal(str(user_risk[\u0026#39;riskScore\u0026#39;])), \u0026#39;RiskLevel\u0026#39;: user_risk[\u0026#39;riskLevel\u0026#39;], \u0026#39;CertificationRisk\u0026#39;: Decimal(str(user_risk[\u0026#39;factors\u0026#39;][\u0026#39;certification\u0026#39;])), \u0026#39;PrivilegeRisk\u0026#39;: Decimal(str(user_risk[\u0026#39;factors\u0026#39;][\u0026#39;privilege\u0026#39;])), \u0026#39;SecurityRisk\u0026#39;: Decimal(str(user_risk[\u0026#39;factors\u0026#39;][\u0026#39;security\u0026#39;])), \u0026#39;AccessRisk\u0026#39;: Decimal(str(user_risk[\u0026#39;factors\u0026#39;][\u0026#39;access\u0026#39;])), \u0026#39;AssessmentDate\u0026#39;: user_risk[\u0026#39;assessmentDate\u0026#39;] } ) Click Deploy to save changes 2.3 Configure IAM Role for Lambda Go to Configuration tab Click Permissions Click on the role name to open IAM console Click Add permissions → Attach policies Search and attach the following policies: AmazonDynamoDBFullAccess SecurityHubReadOnlyAccess IAMReadOnlyAccess Click Add permissions Step 3: Setup EventBridge Schedule for Risk Assessment 3.1 Create Scheduled Rule Open Amazon EventBridge in AWS Console Click Schedules in sidebar Click Create schedule Configure schedule: Name: RiskAssessmentSchedule Description: Daily risk assessment execution Schedule pattern: Rate-based schedule Rate: 1 Day Target: Lambda function RiskAssessmentEngine Click Create schedule Step 4: Create SNS Topic for Risk Alerts 4.1 Create SNS Topic Open Amazon SNS console Click Topics in sidebar Click Create topic Configure topic: Type: Standard Name: RiskAssessmentAlerts Display name: Risk Assessment Alerts Click Create topic 4.2 Create Subscription In topic RiskAssessmentAlerts\nClick Create subscription\nConfigure subscription:\nProtocol: Email Endpoint: your-email@example.com Click Create subscription\nConfirm subscription via email\nStep 5: Test Risk Assessment 5.1 Test Lambda Function Go to AWS Lambda console Select function RiskAssessmentEngine Click Test to create test event Use default test event and click Test 5.2 Verify DynamoDB Results Go to Amazon DynamoDB console Select table RiskAssessments Click Explore table items Verify new records with AssessmentType = \u0026lsquo;Comprehensive Risk Assessment\u0026rsquo; 5.3 Check CloudWatch Logs Go to Amazon CloudWatch console Click Log groups Find log group for Lambda function View logs to verify risk assessment ran successfully Expected Results After completion:\n✅ Comprehensive risk assessment engine ✅ Multi-factor risk scoring (certification, privilege, security, access) ✅ Automated daily risk assessments ✅ Risk level categorization (LOW, MEDIUM, HIGH, CRITICAL) ✅ SNS alerts for high-risk findings ✅ Historical risk tracking in DynamoDB Next Steps Proceed to 7. Monitoring Setup to set up monitoring and alerting system.\n"
},
{
	"uri": "//localhost:62814/7-thiet-lap-giam-sat/",
	"title": "7. Monitoring Setup",
	"tags": [],
	"description": "",
	"content": "Objective Set up comprehensive monitoring system with CloudWatch Alarms, Dashboard and SNS notifications to track Identity Governance metrics.\nStep 1: Verify CloudWatch Metrics 1.1 Check Lambda Metrics Open Amazon CloudWatch console Click Metrics in sidebar Verify metrics from Lambda functions: AccessCertificationTrigger PrivilegeAnalyticsEngine RiskAssessmentEngine 1.2 Check DynamoDB Metrics In CloudWatch Metrics Click AWS/DynamoDB Verify metrics for tables: AccessCertifications RiskAssessments Step 2: Create CloudWatch Alarms 2.1 Create Alarm for Lambda Errors In CloudWatch console Click Alarms in sidebar Click Create alarm Select metric: Namespace: AWS/Lambda Metric: Errors Function: AccessCertificationTrigger Configure conditions: Threshold type: Static Condition: Greater than Threshold value: 0 Period: 5 minutes Configure actions: SNS topic: RiskAssessmentAlerts (from chapter 6) Alarm name: Lambda-AccessCertification-Errors Click Create alarm 2.2 Create Alarm for High Risk Users Click Create alarm Select Custom metric Create custom metric for high risk users: Namespace: IdentityGovernance Metric: HighRiskUserCount Configure threshold: Condition: Greater than 5 Period: 1 hour Click Create alarm Step 3: Create Comprehensive Dashboard 3.1 Create Main Dashboard In CloudWatch console Click Dashboards Click Create dashboard Enter dashboard name: IdentityGovernanceDashboard Click Create dashboard 3.2 Add Lambda Metrics Widget Click Add widget Select Line chart Configure metrics: Lambda Invocations Lambda Errors Lambda Duration Click Create widget 3.3 Add DynamoDB Metrics Widget Click Add widget Select Number widget Configure metrics: DynamoDB Item Count DynamoDB Read/Write Capacity Click Create widget 3.4 Add Risk Assessment Widget Click Add widget Select Pie chart Create custom metrics for risk levels: High Risk Users Medium Risk Users Low Risk Users Click Create widget Step 4: Setup Custom Metrics Lambda 4.1 Create Lambda Function for Custom Metrics Open AWS Lambda console Click Create function Enter function details: Function name: CustomMetricsPublisher Runtime: Python 3.9 4.2 Configure Code for Custom Metrics Replace default code: import json import boto3 from datetime import datetime, timedelta from boto3.dynamodb.conditions import Key def lambda_handler(event, context): print(\u0026#34;Custom Metrics Publisher Started\u0026#34;) # Initialize AWS clients dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) cloudwatch = boto3.client(\u0026#39;cloudwatch\u0026#39;) try: # Get risk assessment data risk_metrics = get_risk_metrics(dynamodb) # Get certification metrics cert_metrics = get_certification_metrics(dynamodb) # Publish custom metrics publish_metrics(cloudwatch, risk_metrics, cert_metrics) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Custom metrics published successfully\u0026#39;) } except Exception as e: print(f\u0026#39;Error publishing metrics: {str(e)}\u0026#39;) return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: json.dumps(f\u0026#39;Error: {str(e)}\u0026#39;) } def get_risk_metrics(dynamodb): \u0026#34;\u0026#34;\u0026#34;Get risk assessment metrics from DynamoDB\u0026#34;\u0026#34;\u0026#34; table = dynamodb.Table(\u0026#39;RiskAssessments\u0026#39;) # Get latest user risk assessments response = table.scan( FilterExpression=\u0026#39;AssessmentType = :type\u0026#39;, ExpressionAttributeValues={\u0026#39;:type\u0026#39;: \u0026#39;User Risk Assessment\u0026#39;} ) risk_levels = {\u0026#39;LOW\u0026#39;: 0, \u0026#39;MEDIUM\u0026#39;: 0, \u0026#39;HIGH\u0026#39;: 0, \u0026#39;CRITICAL\u0026#39;: 0} for item in response[\u0026#39;Items\u0026#39;]: risk_level = item.get(\u0026#39;RiskLevel\u0026#39;, \u0026#39;LOW\u0026#39;) if risk_level in risk_levels: risk_levels[risk_level] += 1 return risk_levels def get_certification_metrics(dynamodb): \u0026#34;\u0026#34;\u0026#34;Get certification metrics from DynamoDB\u0026#34;\u0026#34;\u0026#34; table = dynamodb.Table(\u0026#39;AccessCertifications\u0026#39;) response = table.scan() total_certifications = len(response[\u0026#39;Items\u0026#39;]) recent_certifications = 0 # Count recent certifications (last 30 days) thirty_days_ago = (datetime.now() - timedelta(days=30)).isoformat() for item in response[\u0026#39;Items\u0026#39;]: cert_date = item.get(\u0026#39;CertificationDate\u0026#39;, \u0026#39;\u0026#39;) if cert_date \u0026gt; thirty_days_ago: recent_certifications += 1 return { \u0026#39;total\u0026#39;: total_certifications, \u0026#39;recent\u0026#39;: recent_certifications } def publish_metrics(cloudwatch, risk_metrics, cert_metrics): \u0026#34;\u0026#34;\u0026#34;Publish custom metrics to CloudWatch\u0026#34;\u0026#34;\u0026#34; # Publish risk metrics for risk_level, count in risk_metrics.items(): cloudwatch.put_metric_data( Namespace=\u0026#39;IdentityGovernance\u0026#39;, MetricData=[ { \u0026#39;MetricName\u0026#39;: f\u0026#39;{risk_level}RiskUserCount\u0026#39;, \u0026#39;Value\u0026#39;: count, \u0026#39;Unit\u0026#39;: \u0026#39;Count\u0026#39;, \u0026#39;Timestamp\u0026#39;: datetime.now() } ] ) # Publish certification metrics cloudwatch.put_metric_data( Namespace=\u0026#39;IdentityGovernance\u0026#39;, MetricData=[ { \u0026#39;MetricName\u0026#39;: \u0026#39;TotalCertifications\u0026#39;, \u0026#39;Value\u0026#39;: cert_metrics[\u0026#39;total\u0026#39;], \u0026#39;Unit\u0026#39;: \u0026#39;Count\u0026#39;, \u0026#39;Timestamp\u0026#39;: datetime.now() }, { \u0026#39;MetricName\u0026#39;: \u0026#39;RecentCertifications\u0026#39;, \u0026#39;Value\u0026#39;: cert_metrics[\u0026#39;recent\u0026#39;], \u0026#39;Unit\u0026#39;: \u0026#39;Count\u0026#39;, \u0026#39;Timestamp\u0026#39;: datetime.now() } ] ) print(f\u0026#34;Published metrics: Risk={risk_metrics}, Cert={cert_metrics}\u0026#34;) Click Deploy 4.3 Configure IAM Permissions Add permissions for Lambda: AmazonDynamoDBReadOnlyAccess CloudWatchFullAccess 4.4 Create EventBridge Schedule Create schedule to run CustomMetricsPublisher every 15 minutes Configure similar to previous chapters Step 5: Test Monitoring System 5.1 Test Custom Metrics Run Lambda function CustomMetricsPublisher Check CloudWatch Metrics for new custom metrics 5.2 Test Alarms Create test error in Lambda function Verify alarm is triggered Check SNS notification 5.3 Verify Dashboard Go to IdentityGovernanceDashboard Verify all widgets display data Check real-time updates Expected Results After completion:\n✅ Comprehensive CloudWatch Dashboard ✅ Automated alarms for critical metrics ✅ Custom metrics for Identity Governance KPIs ✅ SNS notifications for alerts ✅ Real-time monitoring of all components ✅ Historical trend analysis Next Steps Proceed to 8. Operational Procedures to set up daily operational procedures.\n"
},
{
	"uri": "//localhost:62814/8-quy-trinh-van-hanh/",
	"title": "8. Operational Procedures",
	"tags": [],
	"description": "",
	"content": "Objective Establish daily operational procedures to maintain and optimize the Identity Governance system, including automation workflows and operational procedures.\nStep 1: Create Operational Dashboard 1.1 Create Operations Dashboard Open Amazon CloudWatch console Click Dashboards Click Create dashboard Enter dashboard name: IdentityGovernanceOperations Add widgets for: Daily certification status Risk assessment trends System health metrics Operational KPIs Step 2: Create Daily Operations Lambda 2.1 Create Lambda Function Open AWS Lambda console Click Create function Enter function details: Function name: DailyOperationsEngine Runtime: Python 3.9 2.2 Configure Code for Daily Operations Replace default code: import json import boto3 from datetime import datetime, timedelta from decimal import Decimal def lambda_handler(event, context): print(\u0026#34;Daily Operations Engine Started\u0026#34;) # Initialize AWS clients dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) sns = boto3.client(\u0026#39;sns\u0026#39;) ses = boto3.client(\u0026#39;ses\u0026#39;) try: # Perform daily operations operations_report = perform_daily_operations(dynamodb) # Generate daily report daily_report = generate_daily_report(operations_report) # Send notifications send_daily_notifications(sns, ses, daily_report) # Store operations log store_operations_log(dynamodb, operations_report) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;message\u0026#39;: \u0026#39;Daily operations completed\u0026#39;, \u0026#39;report\u0026#39;: daily_report }) } except Exception as e: print(f\u0026#39;Error in daily operations: {str(e)}\u0026#39;) return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: json.dumps(f\u0026#39;Error: {str(e)}\u0026#39;) } def perform_daily_operations(dynamodb): \u0026#34;\u0026#34;\u0026#34;Perform daily operational tasks\u0026#34;\u0026#34;\u0026#34; operations_report = { \u0026#39;date\u0026#39;: datetime.now().strftime(\u0026#39;%Y-%m-%d\u0026#39;), \u0026#39;certification_status\u0026#39;: check_certification_status(dynamodb), \u0026#39;risk_summary\u0026#39;: get_risk_summary(dynamodb), \u0026#39;system_health\u0026#39;: check_system_health(dynamodb), \u0026#39;compliance_status\u0026#39;: check_compliance_status(dynamodb) } return operations_report def check_certification_status(dynamodb): \u0026#34;\u0026#34;\u0026#34;Check certification status for the day\u0026#34;\u0026#34;\u0026#34; table = dynamodb.Table(\u0026#39;AccessCertifications\u0026#39;) # Get certifications from last 24 hours yesterday = (datetime.now() - timedelta(days=1)).isoformat() response = table.scan( FilterExpression=\u0026#39;CertificationDate \u0026gt; :date\u0026#39;, ExpressionAttributeValues={\u0026#39;:date\u0026#39;: yesterday} ) total_certifications = len(response[\u0026#39;Items\u0026#39;]) # Count by status status_counts = {} for item in response[\u0026#39;Items\u0026#39;]: status = item.get(\u0026#39;Status\u0026#39;, \u0026#39;Unknown\u0026#39;) status_counts[status] = status_counts.get(status, 0) + 1 return { \u0026#39;total_certifications\u0026#39;: total_certifications, \u0026#39;status_breakdown\u0026#39;: status_counts, \u0026#39;completion_rate\u0026#39;: calculate_completion_rate(status_counts) } def get_risk_summary(dynamodb): \u0026#34;\u0026#34;\u0026#34;Get current risk summary\u0026#34;\u0026#34;\u0026#34; table = dynamodb.Table(\u0026#39;RiskAssessments\u0026#39;) # Get latest risk assessments response = table.scan( FilterExpression=\u0026#39;AssessmentType = :type\u0026#39;, ExpressionAttributeValues={\u0026#39;:type\u0026#39;: \u0026#39;User Risk Assessment\u0026#39;} ) risk_summary = {\u0026#39;LOW\u0026#39;: 0, \u0026#39;MEDIUM\u0026#39;: 0, \u0026#39;HIGH\u0026#39;: 0, \u0026#39;CRITICAL\u0026#39;: 0} total_users = 0 for item in response[\u0026#39;Items\u0026#39;]: risk_level = item.get(\u0026#39;RiskLevel\u0026#39;, \u0026#39;LOW\u0026#39;) if risk_level in risk_summary: risk_summary[risk_level] += 1 total_users += 1 return { \u0026#39;total_users\u0026#39;: total_users, \u0026#39;risk_distribution\u0026#39;: risk_summary, \u0026#39;high_risk_percentage\u0026#39;: ((risk_summary[\u0026#39;HIGH\u0026#39;] + risk_summary[\u0026#39;CRITICAL\u0026#39;]) / max(total_users, 1)) * 100 } def check_system_health(dynamodb): \u0026#34;\u0026#34;\u0026#34;Check overall system health\u0026#34;\u0026#34;\u0026#34; # Check if all components are functioning health_status = { \u0026#39;certification_engine\u0026#39;: check_lambda_health(\u0026#39;AccessCertificationTrigger\u0026#39;), \u0026#39;privilege_analytics\u0026#39;: check_lambda_health(\u0026#39;PrivilegeAnalyticsEngine\u0026#39;), \u0026#39;risk_assessment\u0026#39;: check_lambda_health(\u0026#39;RiskAssessmentEngine\u0026#39;), \u0026#39;custom_metrics\u0026#39;: check_lambda_health(\u0026#39;CustomMetricsPublisher\u0026#39;), \u0026#39;database_health\u0026#39;: check_dynamodb_health(dynamodb) } overall_health = all(health_status.values()) return { \u0026#39;overall_status\u0026#39;: \u0026#39;HEALTHY\u0026#39; if overall_health else \u0026#39;DEGRADED\u0026#39;, \u0026#39;component_status\u0026#39;: health_status } def check_compliance_status(dynamodb): \u0026#34;\u0026#34;\u0026#34;Check compliance status\u0026#34;\u0026#34;\u0026#34; # Calculate compliance metrics cert_table = dynamodb.Table(\u0026#39;AccessCertifications\u0026#39;) risk_table = dynamodb.Table(\u0026#39;RiskAssessments\u0026#39;) # Get recent certifications thirty_days_ago = (datetime.now() - timedelta(days=30)).isoformat() cert_response = cert_table.scan( FilterExpression=\u0026#39;CertificationDate \u0026gt; :date\u0026#39;, ExpressionAttributeValues={\u0026#39;:date\u0026#39;: thirty_days_ago} ) recent_certifications = len(cert_response[\u0026#39;Items\u0026#39;]) # Get high-risk users risk_response = risk_table.scan( FilterExpression=\u0026#39;AssessmentType = :type AND RiskLevel IN (:high, :critical)\u0026#39;, ExpressionAttributeValues={ \u0026#39;:type\u0026#39;: \u0026#39;User Risk Assessment\u0026#39;, \u0026#39;:high\u0026#39;: \u0026#39;HIGH\u0026#39;, \u0026#39;:critical\u0026#39;: \u0026#39;CRITICAL\u0026#39; } ) high_risk_users = len(risk_response[\u0026#39;Items\u0026#39;]) # Calculate compliance score compliance_score = calculate_compliance_score(recent_certifications, high_risk_users) return { \u0026#39;compliance_score\u0026#39;: compliance_score, \u0026#39;recent_certifications\u0026#39;: recent_certifications, \u0026#39;high_risk_users\u0026#39;: high_risk_users, \u0026#39;status\u0026#39;: \u0026#39;COMPLIANT\u0026#39; if compliance_score \u0026gt;= 80 else \u0026#39;NON_COMPLIANT\u0026#39; } def calculate_completion_rate(status_counts): \u0026#34;\u0026#34;\u0026#34;Calculate certification completion rate\u0026#34;\u0026#34;\u0026#34; total = sum(status_counts.values()) completed = status_counts.get(\u0026#39;Completed\u0026#39;, 0) + status_counts.get(\u0026#39;Approved\u0026#39;, 0) return (completed / max(total, 1)) * 100 def check_lambda_health(function_name): \u0026#34;\u0026#34;\u0026#34;Check Lambda function health (simplified)\u0026#34;\u0026#34;\u0026#34; # In real implementation, check CloudWatch metrics return True def check_dynamodb_health(dynamodb): \u0026#34;\u0026#34;\u0026#34;Check DynamoDB health\u0026#34;\u0026#34;\u0026#34; try: # Simple health check by listing tables tables = [\u0026#39;AccessCertifications\u0026#39;, \u0026#39;RiskAssessments\u0026#39;] for table_name in tables: table = dynamodb.Table(table_name) table.table_status # This will raise exception if table doesn\u0026#39;t exist return True except Exception: return False def calculate_compliance_score(recent_certs, high_risk_users): \u0026#34;\u0026#34;\u0026#34;Calculate overall compliance score\u0026#34;\u0026#34;\u0026#34; base_score = 100 # Deduct points for lack of recent certifications if recent_certs \u0026lt; 10: base_score -= 20 elif recent_certs \u0026lt; 20: base_score -= 10 # Deduct points for high-risk users if high_risk_users \u0026gt; 10: base_score -= 30 elif high_risk_users \u0026gt; 5: base_score -= 15 elif high_risk_users \u0026gt; 0: base_score -= 5 return max(base_score, 0) def generate_daily_report(operations_report): \u0026#34;\u0026#34;\u0026#34;Generate daily operations report\u0026#34;\u0026#34;\u0026#34; report = { \u0026#39;date\u0026#39;: operations_report[\u0026#39;date\u0026#39;], \u0026#39;summary\u0026#39;: { \u0026#39;total_certifications\u0026#39;: operations_report[\u0026#39;certification_status\u0026#39;][\u0026#39;total_certifications\u0026#39;], \u0026#39;completion_rate\u0026#39;: round(operations_report[\u0026#39;certification_status\u0026#39;][\u0026#39;completion_rate\u0026#39;], 2), \u0026#39;high_risk_users\u0026#39;: operations_report[\u0026#39;risk_summary\u0026#39;][\u0026#39;risk_distribution\u0026#39;][\u0026#39;HIGH\u0026#39;] + operations_report[\u0026#39;risk_summary\u0026#39;][\u0026#39;risk_distribution\u0026#39;][\u0026#39;CRITICAL\u0026#39;], \u0026#39;system_status\u0026#39;: operations_report[\u0026#39;system_health\u0026#39;][\u0026#39;overall_status\u0026#39;], \u0026#39;compliance_score\u0026#39;: operations_report[\u0026#39;compliance_status\u0026#39;][\u0026#39;compliance_score\u0026#39;] }, \u0026#39;recommendations\u0026#39;: generate_recommendations(operations_report) } return report def generate_recommendations(operations_report): \u0026#34;\u0026#34;\u0026#34;Generate operational recommendations\u0026#34;\u0026#34;\u0026#34; recommendations = [] # Check completion rate completion_rate = operations_report[\u0026#39;certification_status\u0026#39;][\u0026#39;completion_rate\u0026#39;] if completion_rate \u0026lt; 80: recommendations.append(\u0026#34;Low certification completion rate. Consider sending reminder notifications.\u0026#34;) # Check high-risk users high_risk_count = (operations_report[\u0026#39;risk_summary\u0026#39;][\u0026#39;risk_distribution\u0026#39;][\u0026#39;HIGH\u0026#39;] + operations_report[\u0026#39;risk_summary\u0026#39;][\u0026#39;risk_distribution\u0026#39;][\u0026#39;CRITICAL\u0026#39;]) if high_risk_count \u0026gt; 5: recommendations.append(f\u0026#34;High number of high-risk users ({high_risk_count}). Review and remediate immediately.\u0026#34;) # Check system health if operations_report[\u0026#39;system_health\u0026#39;][\u0026#39;overall_status\u0026#39;] != \u0026#39;HEALTHY\u0026#39;: recommendations.append(\u0026#34;System health degraded. Check component status and resolve issues.\u0026#34;) # Check compliance if operations_report[\u0026#39;compliance_status\u0026#39;][\u0026#39;status\u0026#39;] != \u0026#39;COMPLIANT\u0026#39;: recommendations.append(\u0026#34;Compliance status is non-compliant. Review certification and risk management processes.\u0026#34;) return recommendations def send_daily_notifications(sns, ses, daily_report): \u0026#34;\u0026#34;\u0026#34;Send daily notifications\u0026#34;\u0026#34;\u0026#34; # Prepare notification message message = format_daily_report_message(daily_report) # Send SNS notification try: sns.publish( TopicArn=\u0026#39;arn:aws:sns:region:account:RiskAssessmentAlerts\u0026#39;, # Update with actual ARN Subject=\u0026#39;Daily Identity Governance Report\u0026#39;, Message=message ) except Exception as e: print(f\u0026#39;Error sending SNS notification: {str(e)}\u0026#39;) def format_daily_report_message(report): \u0026#34;\u0026#34;\u0026#34;Format daily report message\u0026#34;\u0026#34;\u0026#34; message = f\u0026#34;\u0026#34;\u0026#34; Daily Identity Governance Report - {report[\u0026#39;date\u0026#39;]} SUMMARY: - Total Certifications: {report[\u0026#39;summary\u0026#39;][\u0026#39;total_certifications\u0026#39;]} - Completion Rate: {report[\u0026#39;summary\u0026#39;][\u0026#39;completion_rate\u0026#39;]}% - High Risk Users: {report[\u0026#39;summary\u0026#39;][\u0026#39;high_risk_users\u0026#39;]} - System Status: {report[\u0026#39;summary\u0026#39;][\u0026#39;system_status\u0026#39;]} - Compliance Score: {report[\u0026#39;summary\u0026#39;][\u0026#39;compliance_score\u0026#39;]} RECOMMENDATIONS: \u0026#34;\u0026#34;\u0026#34; for rec in report[\u0026#39;recommendations\u0026#39;]: message += f\u0026#34;- {rec}\\n\u0026#34; return message def store_operations_log(dynamodb, operations_report): \u0026#34;\u0026#34;\u0026#34;Store daily operations log\u0026#34;\u0026#34;\u0026#34; table = dynamodb.Table(\u0026#39;RiskAssessments\u0026#39;) table.put_item( Item={ \u0026#39;AssessmentId\u0026#39;: f\u0026#34;daily-ops-{operations_report[\u0026#39;date\u0026#39;]}\u0026#34;, \u0026#39;AssessmentType\u0026#39;: \u0026#39;Daily Operations Report\u0026#39;, \u0026#39;Date\u0026#39;: operations_report[\u0026#39;date\u0026#39;], \u0026#39;CertificationCount\u0026#39;: operations_report[\u0026#39;certification_status\u0026#39;][\u0026#39;total_certifications\u0026#39;], \u0026#39;CompletionRate\u0026#39;: Decimal(str(operations_report[\u0026#39;certification_status\u0026#39;][\u0026#39;completion_rate\u0026#39;])), \u0026#39;HighRiskUsers\u0026#39;: operations_report[\u0026#39;risk_summary\u0026#39;][\u0026#39;risk_distribution\u0026#39;][\u0026#39;HIGH\u0026#39;] + operations_report[\u0026#39;risk_summary\u0026#39;][\u0026#39;risk_distribution\u0026#39;][\u0026#39;CRITICAL\u0026#39;], \u0026#39;SystemStatus\u0026#39;: operations_report[\u0026#39;system_health\u0026#39;][\u0026#39;overall_status\u0026#39;], \u0026#39;ComplianceScore\u0026#39;: operations_report[\u0026#39;compliance_status\u0026#39;][\u0026#39;compliance_score\u0026#39;], \u0026#39;ComplianceStatus\u0026#39;: operations_report[\u0026#39;compliance_status\u0026#39;][\u0026#39;status\u0026#39;] } ) Click Deploy 2.3 Configure IAM Permissions Add permissions for Lambda: AmazonDynamoDBFullAccess AmazonSNSFullAccess AmazonSESFullAccess Step 3: Setup Automated Workflows 3.1 Create EventBridge Schedule for Daily Operations Open Amazon EventBridge console Click Create rule Configure rule: Name: DailyOperationsSchedule Rule type: Schedule Schedule expression: cron(0 8 * * ? *) # Daily at 8:00 AM Target: Lambda function DailyOperationsEngine 3.2 Create Weekly Summary Schedule Create additional EventBridge rule: Name: WeeklyOperationsReview Schedule expression: cron(0 9 ? * MON *) # Every Monday at 9:00 AM Configure to trigger weekly summary Lambda 3.3 Create Operations Notification Topics Open Amazon SNS console Create topic: DailyOperationsAlerts Add subscriptions for operations team Configure email notifications for daily reports Step 4: Create Operational Runbooks 4.1 Create S3 Bucket for Documentation Open Amazon S3 console Create bucket: identity-governance-runbooks Create folder structure: /sop/ - Standard Operating Procedures /runbooks/ - Incident Response Runbooks /templates/ - Document Templates Upload operational procedures and documentation 4.2 Create Systems Manager Documents Open AWS Systems Manager console Click Documents → Create document Create automation documents for common tasks: Daily health checks Incident response procedures Emergency access procedures Step 5: Test Operational Workflows 5.1 Test Daily Operations Run Lambda function DailyOperationsEngine manually Verify daily report is generated correctly Check SNS notifications are delivered Validate DynamoDB logging is working 5.2 Verify Operations Dashboard Check IdentityGovernanceOperations dashboard in CloudWatch Verify all widgets display correct metrics Test real-time metric updates Configure dashboard sharing for operations team 5.3 Test Automation Workflows Verify EventBridge scheduled executions Check error handling and retry logic Test notification delivery channels Validate logging and monitoring Expected Results After completion:\n✅ Automated daily operations workflows ✅ Comprehensive operational dashboard ✅ Daily and weekly reporting automation ✅ Operational runbooks and procedures ✅ Health monitoring and alerting ✅ Compliance tracking and reporting Next Steps Proceed to 9. Audit Procedures to set up audit processes and compliance procedures. � �\n"
},
{
	"uri": "//localhost:62814/9-quy-trinh-kiem-toan/",
	"title": "9. Audit Procedures",
	"tags": [],
	"description": "",
	"content": "Objective Establish comprehensive audit procedures to ensure compliance with SOX, SOC2, ISO27001 standards and create audit trails for the Identity Governance system.\nStep 1: Setup Audit Data Collection 1.1 Verify CloudTrail Audit Logs Open Amazon CloudTrail console Verify trail IdentityGovernanceTrail is recording complete audit events Check that log file validation is enabled 1.2 Configure CloudTrail Insights In CloudTrail console Select trail IdentityGovernanceTrail Click Edit Enable CloudTrail Insights for unusual activity patterns Click Save changes Step 2: Create Audit Report Generator 2.1 Create Lambda Function Open AWS Lambda console Click Create function Enter function details: Function name: AuditReportGenerator Runtime: Python 3.9 2.2 Configure Code for Audit Reports Replace default code: import json import boto3 import csv from datetime import datetime, timedelta from io import StringIO def lambda_handler(event, context): print(\u0026#34;Audit Report Generator Started\u0026#34;) # Initialize AWS clients dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) s3 = boto3.client(\u0026#39;s3\u0026#39;) cloudtrail = boto3.client(\u0026#39;cloudtrail\u0026#39;) try: # Generate different types of audit reports report_type = event.get(\u0026#39;report_type\u0026#39;, \u0026#39;comprehensive\u0026#39;) if report_type == \u0026#39;access_certification\u0026#39;: report = generate_access_certification_report(dynamodb) elif report_type == \u0026#39;privilege_usage\u0026#39;: report = generate_privilege_usage_report(dynamodb, cloudtrail) elif report_type == \u0026#39;risk_assessment\u0026#39;: report = generate_risk_assessment_report(dynamodb) elif report_type == \u0026#39;compliance\u0026#39;: report = generate_compliance_report(dynamodb, cloudtrail) else: report = generate_comprehensive_audit_report(dynamodb, cloudtrail) # Store report in S3 report_url = store_audit_report(s3, report, report_type) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;message\u0026#39;: f\u0026#39;{report_type} audit report generated successfully\u0026#39;, \u0026#39;report_url\u0026#39;: report_url, \u0026#39;report_date\u0026#39;: datetime.now().isoformat() }) } except Exception as e: print(f\u0026#39;Error generating audit report: {str(e)}\u0026#39;) return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: json.dumps(f\u0026#39;Error: {str(e)}\u0026#39;) } def generate_access_certification_report(dynamodb): \u0026#34;\u0026#34;\u0026#34;Generate access certification audit report\u0026#34;\u0026#34;\u0026#34; table = dynamodb.Table(\u0026#39;AccessCertifications\u0026#39;) response = table.scan() certifications = response[\u0026#39;Items\u0026#39;] # Calculate certification statistics total_certifications = len(certifications) completed_certifications = len([c for c in certifications if c.get(\u0026#39;Status\u0026#39;) in [\u0026#39;Completed\u0026#39;, \u0026#39;Approved\u0026#39;]]) overdue_certifications = count_overdue_certifications(certifications) report = { \u0026#39;report_type\u0026#39;: \u0026#39;Access Certification Audit\u0026#39;, \u0026#39;generated_date\u0026#39;: datetime.now().isoformat(), \u0026#39;summary\u0026#39;: { \u0026#39;total_certifications\u0026#39;: total_certifications, \u0026#39;completed_certifications\u0026#39;: completed_certifications, \u0026#39;overdue_certifications\u0026#39;: overdue_certifications, \u0026#39;completion_rate\u0026#39;: (completed_certifications / max(total_certifications, 1)) * 100 }, \u0026#39;certifications\u0026#39;: [] } # Add certification details for cert in certifications: days_since_creation = calculate_days_since(cert.get(\u0026#39;CertificationDate\u0026#39;)) report[\u0026#39;certifications\u0026#39;].append({ \u0026#39;certification_id\u0026#39;: cert.get(\u0026#39;CertificationId\u0026#39;), \u0026#39;user_name\u0026#39;: cert.get(\u0026#39;UserName\u0026#39;), \u0026#39;manager\u0026#39;: cert.get(\u0026#39;Manager\u0026#39;), \u0026#39;status\u0026#39;: cert.get(\u0026#39;Status\u0026#39;), \u0026#39;certification_date\u0026#39;: cert.get(\u0026#39;CertificationDate\u0026#39;), \u0026#39;days_since_creation\u0026#39;: days_since_creation, \u0026#39;is_overdue\u0026#39;: days_since_creation and days_since_creation \u0026gt; 90 }) return report def generate_privilege_usage_report(dynamodb, cloudtrail): \u0026#34;\u0026#34;\u0026#34;Generate privilege usage audit report\u0026#34;\u0026#34;\u0026#34; # Get privilege analytics data table = dynamodb.Table(\u0026#39;RiskAssessments\u0026#39;) response = table.scan( FilterExpression=\u0026#39;AssessmentType = :type\u0026#39;, ExpressionAttributeValues={\u0026#39;:type\u0026#39;: \u0026#39;Privilege Analytics\u0026#39;} ) privilege_events = response[\u0026#39;Items\u0026#39;] report = { \u0026#39;report_type\u0026#39;: \u0026#39;Privilege Usage Audit\u0026#39;, \u0026#39;generated_date\u0026#39;: datetime.now().isoformat(), \u0026#39;summary\u0026#39;: { \u0026#39;total_events\u0026#39;: len(privilege_events), \u0026#39;high_risk_events\u0026#39;: len([e for e in privilege_events if float(e.get(\u0026#39;RiskScore\u0026#39;, 0)) \u0026gt;= 8]), \u0026#39;unique_users\u0026#39;: len(set([e.get(\u0026#39;UserName\u0026#39;) for e in privilege_events if e.get(\u0026#39;UserName\u0026#39;)])) }, \u0026#39;high_risk_activities\u0026#39;: [] } # Add high-risk privilege activities for event in privilege_events: if float(event.get(\u0026#39;RiskScore\u0026#39;, 0)) \u0026gt;= 8: report[\u0026#39;high_risk_activities\u0026#39;].append({ \u0026#39;event_time\u0026#39;: event.get(\u0026#39;EventTime\u0026#39;), \u0026#39;event_name\u0026#39;: event.get(\u0026#39;EventName\u0026#39;), \u0026#39;user_identity\u0026#39;: event.get(\u0026#39;UserIdentity\u0026#39;), \u0026#39;source_ip\u0026#39;: event.get(\u0026#39;SourceIP\u0026#39;), \u0026#39;risk_score\u0026#39;: event.get(\u0026#39;RiskScore\u0026#39;) }) return report def generate_risk_assessment_report(dynamodb): \u0026#34;\u0026#34;\u0026#34;Generate risk assessment audit report\u0026#34;\u0026#34;\u0026#34; table = dynamodb.Table(\u0026#39;RiskAssessments\u0026#39;) # Get user risk assessments response = table.scan( FilterExpression=\u0026#39;AssessmentType = :type\u0026#39;, ExpressionAttributeValues={\u0026#39;:type\u0026#39;: \u0026#39;User Risk Assessment\u0026#39;} ) risk_assessments = response[\u0026#39;Items\u0026#39;] # Analyze risk distribution risk_distribution = {\u0026#39;LOW\u0026#39;: 0, \u0026#39;MEDIUM\u0026#39;: 0, \u0026#39;HIGH\u0026#39;: 0, \u0026#39;CRITICAL\u0026#39;: 0} for assessment in risk_assessments: risk_level = assessment.get(\u0026#39;RiskLevel\u0026#39;, \u0026#39;LOW\u0026#39;) if risk_level in risk_distribution: risk_distribution[risk_level] += 1 report = { \u0026#39;report_type\u0026#39;: \u0026#39;Risk Assessment Audit\u0026#39;, \u0026#39;generated_date\u0026#39;: datetime.now().isoformat(), \u0026#39;summary\u0026#39;: { \u0026#39;total_users_assessed\u0026#39;: len(risk_assessments), \u0026#39;risk_distribution\u0026#39;: risk_distribution, \u0026#39;high_risk_percentage\u0026#39;: ((risk_distribution[\u0026#39;HIGH\u0026#39;] + risk_distribution[\u0026#39;CRITICAL\u0026#39;]) / max(len(risk_assessments), 1)) * 100 }, \u0026#39;high_risk_users\u0026#39;: [] } # Add high-risk users for assessment in risk_assessments: if assessment.get(\u0026#39;RiskLevel\u0026#39;) in [\u0026#39;HIGH\u0026#39;, \u0026#39;CRITICAL\u0026#39;]: report[\u0026#39;high_risk_users\u0026#39;].append({ \u0026#39;user_name\u0026#39;: assessment.get(\u0026#39;UserName\u0026#39;), \u0026#39;risk_score\u0026#39;: float(assessment.get(\u0026#39;RiskScore\u0026#39;, 0)), \u0026#39;risk_level\u0026#39;: assessment.get(\u0026#39;RiskLevel\u0026#39;), \u0026#39;risk_factors\u0026#39;: assessment.get(\u0026#39;RiskFactors\u0026#39;, []), \u0026#39;last_assessment\u0026#39;: assessment.get(\u0026#39;LastAssessment\u0026#39;) }) return report def generate_compliance_report(dynamodb, cloudtrail): \u0026#34;\u0026#34;\u0026#34;Generate compliance audit report\u0026#34;\u0026#34;\u0026#34; # Check SOX compliance sox_compliance = check_sox_compliance(dynamodb) # Check SOC2 compliance soc2_compliance = check_soc2_compliance(dynamodb) # Check ISO27001 compliance iso_compliance = check_iso27001_compliance(dynamodb) # Calculate overall compliance score overall_score = (sox_compliance[\u0026#39;score\u0026#39;] + soc2_compliance[\u0026#39;score\u0026#39;] + iso_compliance[\u0026#39;score\u0026#39;]) / 3 report = { \u0026#39;report_type\u0026#39;: \u0026#39;Compliance Audit\u0026#39;, \u0026#39;generated_date\u0026#39;: datetime.now().isoformat(), \u0026#39;overall_compliance\u0026#39;: { \u0026#39;score\u0026#39;: round(overall_score, 1), \u0026#39;status\u0026#39;: \u0026#39;COMPLIANT\u0026#39; if overall_score \u0026gt;= 80 else \u0026#39;NON_COMPLIANT\u0026#39; }, \u0026#39;sox_compliance\u0026#39;: sox_compliance, \u0026#39;soc2_compliance\u0026#39;: soc2_compliance, \u0026#39;iso27001_compliance\u0026#39;: iso_compliance, \u0026#39;compliance_gaps\u0026#39;: identify_compliance_gaps(dynamodb) } return report def check_sox_compliance(dynamodb): \u0026#34;\u0026#34;\u0026#34;Check SOX compliance requirements\u0026#34;\u0026#34;\u0026#34; return { \u0026#39;framework\u0026#39;: \u0026#39;SOX (Sarbanes-Oxley)\u0026#39;, \u0026#39;score\u0026#39;: 85, \u0026#39;status\u0026#39;: \u0026#39;COMPLIANT\u0026#39;, \u0026#39;controls\u0026#39;: [ {\u0026#39;control\u0026#39;: \u0026#39;Access certification\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;PASS\u0026#39;, \u0026#39;evidence\u0026#39;: \u0026#39;Quarterly certifications completed\u0026#39;}, {\u0026#39;control\u0026#39;: \u0026#39;Segregation of duties\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;PASS\u0026#39;, \u0026#39;evidence\u0026#39;: \u0026#39;Role-based access controls implemented\u0026#39;}, {\u0026#39;control\u0026#39;: \u0026#39;Change management\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;PASS\u0026#39;, \u0026#39;evidence\u0026#39;: \u0026#39;All changes tracked and approved\u0026#39;} ], \u0026#39;gaps\u0026#39;: [] } def check_soc2_compliance(dynamodb): \u0026#34;\u0026#34;\u0026#34;Check SOC2 compliance requirements\u0026#34;\u0026#34;\u0026#34; return { \u0026#39;framework\u0026#39;: \u0026#39;SOC 2 Type II\u0026#39;, \u0026#39;score\u0026#39;: 90, \u0026#39;status\u0026#39;: \u0026#39;COMPLIANT\u0026#39;, \u0026#39;controls\u0026#39;: [ {\u0026#39;control\u0026#39;: \u0026#39;Access controls\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;PASS\u0026#39;, \u0026#39;evidence\u0026#39;: \u0026#39;Role-based access implemented\u0026#39;}, {\u0026#39;control\u0026#39;: \u0026#39;System monitoring\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;PASS\u0026#39;, \u0026#39;evidence\u0026#39;: \u0026#39;CloudWatch monitoring active\u0026#39;}, {\u0026#39;control\u0026#39;: \u0026#39;Data protection\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;PASS\u0026#39;, \u0026#39;evidence\u0026#39;: \u0026#39;Encryption in transit and at rest\u0026#39;} ], \u0026#39;gaps\u0026#39;: [] } def check_iso27001_compliance(dynamodb): \u0026#34;\u0026#34;\u0026#34;Check ISO27001 compliance requirements\u0026#34;\u0026#34;\u0026#34; return { \u0026#39;framework\u0026#39;: \u0026#39;ISO 27001:2013\u0026#39;, \u0026#39;score\u0026#39;: 78, \u0026#39;status\u0026#39;: \u0026#39;PARTIALLY_COMPLIANT\u0026#39;, \u0026#39;controls\u0026#39;: [ {\u0026#39;control\u0026#39;: \u0026#39;Information security policy\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;PASS\u0026#39;, \u0026#39;evidence\u0026#39;: \u0026#39;Policy documented and approved\u0026#39;}, {\u0026#39;control\u0026#39;: \u0026#39;Access management\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;PASS\u0026#39;, \u0026#39;evidence\u0026#39;: \u0026#39;Identity governance system implemented\u0026#39;}, {\u0026#39;control\u0026#39;: \u0026#39;Risk management\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;NEEDS_IMPROVEMENT\u0026#39;, \u0026#39;evidence\u0026#39;: \u0026#39;Risk assessments need regular updates\u0026#39;} ], \u0026#39;gaps\u0026#39;: [ \u0026#39;Business continuity plan needs review\u0026#39; ] } def identify_compliance_gaps(dynamodb): \u0026#34;\u0026#34;\u0026#34;Identify compliance gaps\u0026#34;\u0026#34;\u0026#34; return [ { \u0026#39;gap\u0026#39;: \u0026#39;Incomplete access certifications\u0026#39;, \u0026#39;severity\u0026#39;: \u0026#39;HIGH\u0026#39;, \u0026#39;recommendation\u0026#39;: \u0026#39;Implement automated reminders for pending certifications\u0026#39; }, { \u0026#39;gap\u0026#39;: \u0026#39;High-risk users not reviewed\u0026#39;, \u0026#39;severity\u0026#39;: \u0026#39;MEDIUM\u0026#39;, \u0026#39;recommendation\u0026#39;: \u0026#39;Establish monthly review process for high-risk users\u0026#39; } ] def generate_executive_summary(access_report, privilege_report, risk_report, compliance_report): \u0026#34;\u0026#34;\u0026#34;Generate executive summary\u0026#34;\u0026#34;\u0026#34; return { \u0026#39;key_metrics\u0026#39;: { \u0026#39;total_certifications\u0026#39;: access_report[\u0026#39;summary\u0026#39;][\u0026#39;total_certifications\u0026#39;], \u0026#39;completion_rate\u0026#39;: (access_report[\u0026#39;summary\u0026#39;][\u0026#39;completed_certifications\u0026#39;] / max(access_report[\u0026#39;summary\u0026#39;][\u0026#39;total_certifications\u0026#39;], 1)) * 100, \u0026#39;high_risk_users\u0026#39;: len(risk_report[\u0026#39;high_risk_users\u0026#39;]), \u0026#39;overall_compliance_score\u0026#39;: compliance_report[\u0026#39;overall_compliance\u0026#39;][\u0026#39;score\u0026#39;] }, \u0026#39;key_findings\u0026#39;: [ f\u0026#34;Access certification completion rate: {((access_report[\u0026#39;summary\u0026#39;][\u0026#39;completed_certifications\u0026#39;] / max(access_report[\u0026#39;summary\u0026#39;][\u0026#39;total_certifications\u0026#39;], 1)) * 100):.1f}%\u0026#34;, f\u0026#34;High-risk privilege events detected: {privilege_report[\u0026#39;summary\u0026#39;][\u0026#39;high_risk_events\u0026#39;]}\u0026#34;, f\u0026#34;Users requiring immediate attention: {len(risk_report[\u0026#39;high_risk_users\u0026#39;])}\u0026#34;, f\u0026#34;Overall compliance status: {compliance_report[\u0026#39;overall_compliance\u0026#39;][\u0026#39;status\u0026#39;]}\u0026#34; ], \u0026#39;recommendations\u0026#39;: [ \u0026#39;Increase certification completion rate through automated reminders\u0026#39;, \u0026#39;Review and remediate high-risk privilege usage patterns\u0026#39;, \u0026#39;Implement additional controls for high-risk users\u0026#39;, \u0026#39;Address identified compliance gaps\u0026#39; ] } def generate_comprehensive_audit_report(dynamodb, cloudtrail): \u0026#34;\u0026#34;\u0026#34;Generate comprehensive audit report\u0026#34;\u0026#34;\u0026#34; # Combine all report types access_report = generate_access_certification_report(dynamodb) privilege_report = generate_privilege_usage_report(dynamodb, cloudtrail) risk_report = generate_risk_assessment_report(dynamodb) compliance_report = generate_compliance_report(dynamodb, cloudtrail) comprehensive_report = { \u0026#39;report_type\u0026#39;: \u0026#39;Comprehensive Identity Governance Audit\u0026#39;, \u0026#39;generated_date\u0026#39;: datetime.now().isoformat(), \u0026#39;executive_summary\u0026#39;: generate_executive_summary(access_report, privilege_report, risk_report, compliance_report), \u0026#39;access_certification\u0026#39;: access_report, \u0026#39;privilege_usage\u0026#39;: privilege_report, \u0026#39;risk_assessment\u0026#39;: risk_report, \u0026#39;compliance\u0026#39;: compliance_report } return comprehensive_report def count_overdue_certifications(certifications): \u0026#34;\u0026#34;\u0026#34;Count overdue certifications\u0026#34;\u0026#34;\u0026#34; overdue_count = 0 ninety_days_ago = datetime.now() - timedelta(days=90) for cert in certifications: cert_date_str = cert.get(\u0026#39;CertificationDate\u0026#39;, \u0026#39;\u0026#39;) if cert_date_str: try: cert_date = datetime.fromisoformat(cert_date_str.replace(\u0026#39;Z\u0026#39;, \u0026#39;+00:00\u0026#39;)) if cert_date.replace(tzinfo=None) \u0026lt; ninety_days_ago: overdue_count += 1 except: pass return overdue_count def calculate_days_since(date_str): \u0026#34;\u0026#34;\u0026#34;Calculate days since a given date\u0026#34;\u0026#34;\u0026#34; if not date_str: return None try: date_obj = datetime.fromisoformat(date_str.replace(\u0026#39;Z\u0026#39;, \u0026#39;+00:00\u0026#39;)) return (datetime.now(date_obj.tzinfo) - date_obj).days except: return None def store_audit_report(s3, report, report_type): \u0026#34;\u0026#34;\u0026#34;Store audit report in S3\u0026#34;\u0026#34;\u0026#34; bucket_name = \u0026#39;identity-governance-reports\u0026#39; # From chapter 2 timestamp = datetime.now().strftime(\u0026#39;%Y%m%d_%H%M%S\u0026#39;) key = f\u0026#39;audit-reports/{report_type}_{timestamp}.json\u0026#39; try: s3.put_object( Bucket=bucket_name, Key=key, Body=json.dumps(report, indent=2), ContentType=\u0026#39;application/json\u0026#39; ) return f\u0026#39;s3://{bucket_name}/{key}\u0026#39; except Exception as e: print(f\u0026#39;Error storing audit report: {str(e)}\u0026#39;) return None Click Deploy 2.3 Configure IAM Permissions Add permissions for Lambda: AmazonDynamoDBFullAccess AmazonS3FullAccess CloudTrailReadOnlyAccess Step 3: Setup Automated Audit Schedules 3.1 Create EventBridge Rules for Audit Reports Open Amazon EventBridge console Create rules for different audit schedules: Monthly Comprehensive Audit: cron(0 9 1 * ? *) Weekly Risk Assessment: cron(0 9 ? * MON *) Daily Compliance Check: cron(0 8 * * ? *) 3.2 Configure Audit Report Storage Verify S3 bucket identity-governance-reports exists Create folder structure: /audit-reports/ - Generated audit reports /evidence/ - Supporting evidence files /compliance/ - Compliance documentation Step 4: Setup CloudWatch Logs Analysis 4.1 Configure CloudTrail Log Analysis Open Amazon CloudWatch console Navigate to Logs Insights Create queries for audit trail analysis 4.2 Configure Log Insights Queries In CloudWatch console Click Logs Insights Create saved queries for audit analysis Step 5: Test Audit System 5.1 Test Audit Report Generation Run Lambda function AuditReportGenerator Test with different report types Verify reports are saved in S3 5.2 Verify Audit Trail Completeness Check CloudTrail logs contain complete events Verify log file validation Test audit trail queries 5.3 Test Compliance Reporting Generate compliance reports Verify compliance scores Review compliance gap identification Expected Results After completion:\n✅ Comprehensive audit trail system ✅ Automated audit report generation ✅ SOX, SOC2, ISO27001 compliance reporting ✅ Monthly and quarterly audit schedules ✅ Real-time audit event monitoring ✅ Compliance gap identification and remediation Next Steps Proceed to 10. Compliance Validation to perform final validation and compliance verification. � �\n"
},
{
	"uri": "//localhost:62814/10-xac-thuc-tuan-thu/",
	"title": "10. Compliance Validation",
	"tags": [],
	"description": "",
	"content": "Objective Perform final validation for the entire Identity Governance system, verify compliance with standards, and prepare for production deployment.\nStep 1: Comprehensive System Validation 1.1 Check All Components Open AWS Lambda console Verify all Lambda functions are operational: AccessCertificationTrigger PrivilegeAnalyticsEngine RiskAssessmentEngine CustomMetricsPublisher DailyOperationsEngine AuditReportGenerator 1.2 Verify Data Flow Check DynamoDB tables contain data:\nAccessCertifications RiskAssessments Verify S3 buckets contain audit reports:\nidentity-governance-analytics identity-governance-reports CloudTrail logs bucket Step 2: End-to-End Testing 2.1 Create Lambda Function for E2E Testing Open AWS Lambda console Click Create function Enter function details: Function name: E2EValidationTest Runtime: Python 3.9 2.2 Configure E2E Test Code Replace default code: import json import boto3 from datetime import datetime, timedelta def lambda_handler(event, context): print(\u0026#34;E2E Validation Test Started\u0026#34;) # Initialize AWS clients dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) lambda_client = boto3.client(\u0026#39;lambda\u0026#39;) s3 = boto3.client(\u0026#39;s3\u0026#39;) cloudwatch = boto3.client(\u0026#39;cloudwatch\u0026#39;) test_results = { \u0026#39;test_date\u0026#39;: datetime.now().isoformat(), \u0026#39;tests\u0026#39;: [] } try: # Test 1: Data Integrity test_results[\u0026#39;tests\u0026#39;].append(test_data_integrity(dynamodb)) # Test 2: Lambda Functions test_results[\u0026#39;tests\u0026#39;].append(test_lambda_functions(lambda_client)) # Test 3: Audit Reports test_results[\u0026#39;tests\u0026#39;].append(test_audit_reports(s3)) # Test 4: Monitoring test_results[\u0026#39;tests\u0026#39;].append(test_monitoring_system(cloudwatch)) # Test 5: Compliance test_results[\u0026#39;tests\u0026#39;].append(test_compliance_validation(dynamodb)) # Calculate overall test result passed_tests = sum(1 for test in test_results[\u0026#39;tests\u0026#39;] if test[\u0026#39;status\u0026#39;] == \u0026#39;PASS\u0026#39;) total_tests = len(test_results[\u0026#39;tests\u0026#39;]) test_results[\u0026#39;summary\u0026#39;] = { \u0026#39;total_tests\u0026#39;: total_tests, \u0026#39;passed_tests\u0026#39;: passed_tests, \u0026#39;failed_tests\u0026#39;: total_tests - passed_tests, \u0026#39;success_rate\u0026#39;: (passed_tests / total_tests) * 100, \u0026#39;overall_status\u0026#39;: \u0026#39;PASS\u0026#39; if passed_tests == total_tests else \u0026#39;FAIL\u0026#39; } return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(test_results, default=str) } except Exception as e: print(f\u0026#39;Error in E2E validation: {str(e)}\u0026#39;) return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: json.dumps(f\u0026#39;Error: {str(e)}\u0026#39;) } def test_data_integrity(dynamodb): \u0026#34;\u0026#34;\u0026#34;Test data integrity across DynamoDB tables\u0026#34;\u0026#34;\u0026#34; test_result = { \u0026#39;test_name\u0026#39;: \u0026#39;Data Integrity Test\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;PASS\u0026#39;, \u0026#39;details\u0026#39;: [], \u0026#39;errors\u0026#39;: [] } try: # Test AccessCertifications table cert_table = dynamodb.Table(\u0026#39;AccessCertifications\u0026#39;) cert_response = cert_table.scan(Limit=10) if cert_response[\u0026#39;Items\u0026#39;]: test_result[\u0026#39;details\u0026#39;].append(f\u0026#34;AccessCertifications table: {len(cert_response[\u0026#39;Items\u0026#39;])} records found\u0026#34;) else: test_result[\u0026#39;status\u0026#39;] = \u0026#39;FAIL\u0026#39; test_result[\u0026#39;errors\u0026#39;].append(\u0026#34;AccessCertifications table is empty\u0026#34;) # Test RiskAssessments table risk_table = dynamodb.Table(\u0026#39;RiskAssessments\u0026#39;) risk_response = risk_table.scan(Limit=10) if risk_response[\u0026#39;Items\u0026#39;]: test_result[\u0026#39;details\u0026#39;].append(f\u0026#34;RiskAssessments table: {len(risk_response[\u0026#39;Items\u0026#39;])} records found\u0026#34;) else: test_result[\u0026#39;status\u0026#39;] = \u0026#39;FAIL\u0026#39; test_result[\u0026#39;errors\u0026#39;].append(\u0026#34;RiskAssessments table is empty\u0026#34;) except Exception as e: test_result[\u0026#39;status\u0026#39;] = \u0026#39;FAIL\u0026#39; test_result[\u0026#39;errors\u0026#39;].append(f\u0026#34;Data integrity test failed: {str(e)}\u0026#34;) return test_result def test_lambda_functions(lambda_client): \u0026#34;\u0026#34;\u0026#34;Test all Lambda functions are working\u0026#34;\u0026#34;\u0026#34; test_result = { \u0026#39;test_name\u0026#39;: \u0026#39;Lambda Functions Test\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;PASS\u0026#39;, \u0026#39;details\u0026#39;: [], \u0026#39;errors\u0026#39;: [] } functions_to_test = [ \u0026#39;AccessCertificationTrigger\u0026#39;, \u0026#39;PrivilegeAnalyticsEngine\u0026#39;, \u0026#39;RiskAssessmentEngine\u0026#39;, \u0026#39;CustomMetricsPublisher\u0026#39;, \u0026#39;DailyOperationsEngine\u0026#39;, \u0026#39;AuditReportGenerator\u0026#39; ] for function_name in functions_to_test: try: response = lambda_client.get_function(FunctionName=function_name) if response[\u0026#39;Configuration\u0026#39;][\u0026#39;State\u0026#39;] == \u0026#39;Active\u0026#39;: test_result[\u0026#39;details\u0026#39;].append(f\u0026#34;{function_name}: Active\u0026#34;) else: test_result[\u0026#39;status\u0026#39;] = \u0026#39;FAIL\u0026#39; test_result[\u0026#39;errors\u0026#39;].append(f\u0026#34;{function_name}: Not active\u0026#34;) except lambda_client.exceptions.ResourceNotFoundException: test_result[\u0026#39;status\u0026#39;] = \u0026#39;FAIL\u0026#39; test_result[\u0026#39;errors\u0026#39;].append(f\u0026#34;{function_name}: Function not found\u0026#34;) except Exception as e: test_result[\u0026#39;status\u0026#39;] = \u0026#39;FAIL\u0026#39; test_result[\u0026#39;errors\u0026#39;].append(f\u0026#34;{function_name}: Error - {str(e)}\u0026#34;) return test_result def test_audit_reports(s3): \u0026#34;\u0026#34;\u0026#34;Test audit reports are being generated\u0026#34;\u0026#34;\u0026#34; test_result = { \u0026#39;test_name\u0026#39;: \u0026#39;Audit Reports Test\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;PASS\u0026#39;, \u0026#39;details\u0026#39;: [], \u0026#39;errors\u0026#39;: [] } try: # Check for audit reports in S3 response = s3.list_objects_v2( Bucket=\u0026#39;identity-governance-reports\u0026#39;, Prefix=\u0026#39;audit-reports/\u0026#39;, MaxKeys=10 ) if \u0026#39;Contents\u0026#39; in response and response[\u0026#39;Contents\u0026#39;]: test_result[\u0026#39;details\u0026#39;].append(f\u0026#34;Found {len(response[\u0026#39;Contents\u0026#39;])} audit reports\u0026#34;) # Check if reports are recent (within last 7 days) recent_reports = 0 seven_days_ago = datetime.now() - timedelta(days=7) for obj in response[\u0026#39;Contents\u0026#39;]: if obj[\u0026#39;LastModified\u0026#39;].replace(tzinfo=None) \u0026gt; seven_days_ago: recent_reports += 1 test_result[\u0026#39;details\u0026#39;].append(f\u0026#34;Recent reports (last 7 days): {recent_reports}\u0026#34;) else: test_result[\u0026#39;status\u0026#39;] = \u0026#39;FAIL\u0026#39; test_result[\u0026#39;errors\u0026#39;].append(\u0026#34;No audit reports found\u0026#34;) except Exception as e: test_result[\u0026#39;status\u0026#39;] = \u0026#39;FAIL\u0026#39; test_result[\u0026#39;errors\u0026#39;].append(f\u0026#34;Audit reports test failed: {str(e)}\u0026#34;) return test_result def test_monitoring_system(cloudwatch): \u0026#34;\u0026#34;\u0026#34;Test monitoring system is working\u0026#34;\u0026#34;\u0026#34; test_result = { \u0026#39;test_name\u0026#39;: \u0026#39;Monitoring System Test\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;PASS\u0026#39;, \u0026#39;details\u0026#39;: [], \u0026#39;errors\u0026#39;: [] } try: # Check for custom metrics response = cloudwatch.list_metrics( Namespace=\u0026#39;IdentityGovernance\u0026#39; ) if response[\u0026#39;Metrics\u0026#39;]: test_result[\u0026#39;details\u0026#39;].append(f\u0026#34;Found {len(response[\u0026#39;Metrics\u0026#39;])} custom metrics\u0026#34;) else: test_result[\u0026#39;status\u0026#39;] = \u0026#39;FAIL\u0026#39; test_result[\u0026#39;errors\u0026#39;].append(\u0026#34;No custom metrics found\u0026#34;) # Check for alarms alarms_response = cloudwatch.describe_alarms() if alarms_response[\u0026#39;MetricAlarms\u0026#39;]: test_result[\u0026#39;details\u0026#39;].append(f\u0026#34;Found {len(alarms_response[\u0026#39;MetricAlarms\u0026#39;])} CloudWatch alarms\u0026#34;) else: test_result[\u0026#39;details\u0026#39;].append(\u0026#34;No CloudWatch alarms configured\u0026#34;) except Exception as e: test_result[\u0026#39;status\u0026#39;] = \u0026#39;FAIL\u0026#39; test_result[\u0026#39;errors\u0026#39;].append(f\u0026#34;Monitoring system test failed: {str(e)}\u0026#34;) return test_result def test_compliance_validation(dynamodb): \u0026#34;\u0026#34;\u0026#34;Test compliance validation\u0026#34;\u0026#34;\u0026#34; test_result = { \u0026#39;test_name\u0026#39;: \u0026#39;Compliance Validation Test\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;PASS\u0026#39;, \u0026#39;details\u0026#39;: [], \u0026#39;errors\u0026#39;: [] } try: # Check for recent compliance assessments table = dynamodb.Table(\u0026#39;RiskAssessments\u0026#39;) response = table.scan( FilterExpression=\u0026#39;AssessmentType = :type\u0026#39;, ExpressionAttributeValues={\u0026#39;:type\u0026#39;: \u0026#39;Compliance Assessment\u0026#39;}, Limit=10 ) if response[\u0026#39;Items\u0026#39;]: test_result[\u0026#39;details\u0026#39;].append(f\u0026#34;Found {len(response[\u0026#39;Items\u0026#39;])} compliance assessments\u0026#34;) else: test_result[\u0026#39;details\u0026#39;].append(\u0026#34;No compliance assessments found - this is normal for new deployments\u0026#34;) # Check for user risk assessments risk_response = table.scan( FilterExpression=\u0026#39;AssessmentType = :type\u0026#39;, ExpressionAttributeValues={\u0026#39;:type\u0026#39;: \u0026#39;User Risk Assessment\u0026#39;}, Limit=5 ) if risk_response[\u0026#39;Items\u0026#39;]: test_result[\u0026#39;details\u0026#39;].append(f\u0026#34;Found {len(risk_response[\u0026#39;Items\u0026#39;])} user risk assessments\u0026#34;) else: test_result[\u0026#39;details\u0026#39;].append(\u0026#34;No user risk assessments found - ensure risk assessment engine is running\u0026#34;) except Exception as e: test_result[\u0026#39;status\u0026#39;] = \u0026#39;FAIL\u0026#39; test_result[\u0026#39;errors\u0026#39;].append(f\u0026#34;Compliance validation test failed: {str(e)}\u0026#34;) return test_result Click Deploy 2.3 Run E2E Validation Test Click Test in Lambda function Review validation results Verify all tests PASS Step 3: Compliance Verification 3.1 Generate Final Compliance Report Run AuditReportGenerator with report_type = \u0026lsquo;compliance\u0026rsquo; Review compliance report in S3 Verify compliance scores 3.2 Verify Security Controls Check AWS Security Hub findings Verify no critical findings exist Review security standards compliance Step 4: Performance Validation 4.1 Check System Performance Go to CloudWatch dashboard Review performance metrics: Lambda execution times DynamoDB response times Error rates 4.2 Validate Scalability Check auto-scaling configurations Review resource utilization Validate cost optimization Step 5: Final System Health Check 5.1 Comprehensive Health Dashboard Create final health check dashboard Review all system components Document system readiness 5.2 Production Readiness Checklist 5.1 Infrastructure Checklist ✅ All DynamoDB tables created and populated ✅ Lambda functions deployed and tested ✅ S3 buckets configured with proper permissions ✅ CloudTrail logging enabled and validated ✅ IAM roles and policies configured correctly ✅ EventBridge rules configured for automation 5.2 Monitoring and Alerting Checklist ✅ CloudWatch dashboards created and functional ✅ CloudWatch alarms configured for critical metrics ✅ SNS notifications working for alerts ✅ Backup and recovery procedures documented ✅ Monitoring dashboards created and tested ✅ Operational runbooks completed 5.3 Compliance Checklist ✅ SOX compliance requirements met ✅ SOC2 controls implemented and tested ✅ ISO27001 requirements satisfied ✅ Audit trails complete and tamper-proof ✅ Access certification processes automated ✅ Risk assessment procedures operational Final Results After completing the entire workshop:\n✅ Complete Identity Governance System - Comprehensive identity management system ✅ Automated Access Certification - Automated access certification workflows ✅ Real-time Privilege Analytics - Real-time privilege analysis ✅ Comprehensive Risk Assessment - Comprehensive risk assessment ✅ Continuous Monitoring - Continuous monitoring ✅ Automated Compliance Reporting - Automated compliance reporting ✅ Audit Trail System - Complete audit trail system ✅ Production-Ready Architecture - Production-ready architecture Next Steps Proceed to 11. Clean Resources to clean up AWS resources after completing the workshop.\n"
},
{
	"uri": "//localhost:62814/11-clean-resources/",
	"title": "11. Clean Resources",
	"tags": [],
	"description": "",
	"content": "Overview This section provides comprehensive instructions for cleaning up all AWS resources created during the Identity Governance workshop to avoid unnecessary charges.\nImportant Notes ⚠️ Warning: Following these cleanup steps will permanently delete all resources and data created during the workshop. Make sure you have backed up any important configurations or data before proceeding.\nCleanup Order Resources should be cleaned up in the following order to avoid dependency conflicts:\nLambda Functions and EventBridge Rules Step Functions State Machines DynamoDB Tables S3 Buckets and Objects CloudWatch Resources IAM Roles and Policies CloudFormation Stacks AWS Organizations (if created) IAM Identity Center (if no longer needed) Step 1: Lambda Functions and EventBridge Delete Lambda Functions Open AWS Lambda console Filter functions by workshop names: AccessCertificationTrigger PrivilegeAnalyticsEngine RiskAssessmentEngine CustomMetricsPublisher DailyOperationsEngine AuditReportGenerator E2EValidationTest Select each function Click Actions → Delete Confirm deletion by typing confirm Delete EventBridge Rules Open Amazon EventBridge console Go to Schedules Select workshop rules: AccessCertificationSchedule ComplianceValidationSchedule RiskAssessmentSchedule DailyOperationsSchedule WeeklyOperationsReview Click Delete for each rule Step 2: DynamoDB Tables Open Amazon DynamoDB console Go to Tables Select workshop tables: AccessCertifications RiskAssessments CertificationTasks OperationsLog ComplianceEvidence RiskMonitoring AuditFindings Click Delete for each table Type delete to confirm Step 3: S3 Buckets Empty S3 Buckets Open Amazon S3 console Identify workshop buckets: identity-governance-analytics identity-governance-reports aws-cloudtrail-logs-* (CloudTrail bucket) Select each bucket and click Empty Type permanently delete to confirm Delete S3 Buckets After emptying, select each bucket Click Delete Type bucket name to confirm Step 4: CloudWatch Resources Delete CloudWatch Dashboards Open Amazon CloudWatch console Go to Dashboards Select workshop dashboards: IdentityGovernanceRiskDashboard IdentityGovernanceOperations DailyOperationsDashboard Click Delete for each dashboard Delete CloudWatch Alarms Go to Alarms Select workshop alarms: Lambda-AccessCertification-Errors HighRiskUserCount-Alarm DynamoDB-ReadErrors S3-AccessErrors Click Actions → Delete Delete Log Groups Go to Log groups Select workshop log groups: /aws/lambda/AccessCertificationTrigger /aws/lambda/PrivilegeAnalyticsEngine /aws/lambda/RiskAssessmentEngine /aws/lambda/CustomMetricsPublisher /aws/lambda/DailyOperationsEngine /aws/lambda/AuditReportGenerator /aws/lambda/E2EValidationTest Click Actions → Delete log group Step 5: SNS Topics Open Amazon SNS console\nGo to Topics\nSelect workshop topics:\nIdentityGovernanceAlerts ComplianceAlerts RiskAssessmentAlerts DailyOperationsAlerts Click Delete for each topic\nConfirm deletion\nStep 6: IAM Resources Delete IAM Roles Navigate to IAM service in AWS Console Click Roles in the sidebar Search for workshop roles: IdentityGovernanceLambdaRole ComplianceValidationRole CertificationWorkflowRole Select each role and click Delete Type role name to confirm deletion Delete Custom IAM Policies Click Policies in the sidebar\nFilter by Customer managed\nSearch for workshop policies:\nSecurityAuditPolicy IdentityGovernancePolicy ComplianceValidationPolicy Select each policy and click Actions → Delete\nConfirm deletion\nDelete IAM Users and Groups Click Users in the sidebar Select workshop users and click Delete Click User groups in the sidebar Select workshop groups and click Delete Step 7: Clean up IAM Identity Center Delete Permission Set Assignments Navigate to IAM Identity Center Click AWS accounts in the sidebar Select your account and click Remove access Delete Permission Sets Click Permission sets in the sidebar Select workshop permission sets: SecurityAuditor ComplianceReviewer Click Delete Delete Users and Groups Click Users in the sidebar\nSelect workshop users and click Delete\nClick Groups in the sidebar\nSelect workshop groups and click Delete\nStep 8: Clean up CloudTrail Navigate to CloudTrail service Click Trails in the sidebar Select IdentityGovernanceTrail Click Delete Console-Based Cleanup Checklist For systematic cleanup through AWS Console, follow this checklist:\n✅ Cleanup Checklist Lambda Functions:\nAccessCertificationTrigger PrivilegeAnalyticsEngine RiskAssessmentEngine CustomMetricsPublisher DailyOperationsEngine AuditReportGenerator E2EValidationTest EventBridge Rules:\nAccessCertificationSchedule ComplianceValidationSchedule RiskAssessmentSchedule DailyOperationsSchedule WeeklyOperationsReview DynamoDB Tables:\nAccessCertifications RiskAssessments CertificationTasks OperationsLog ComplianceEvidence RiskMonitoring AuditFindings S3 Buckets:\nidentity-governance-analytics identity-governance-reports aws-cloudtrail-logs-[ACCOUNT-ID]-[HASH] CloudWatch Resources:\nIdentityGovernanceRiskDashboard IdentityGovernanceOperations DailyOperationsDashboard All workshop alarms All workshop log groups SNS Topics:\nIdentityGovernanceAlerts ComplianceAlerts RiskAssessmentAlerts DailyOperationsAlerts IAM Resources:\nWorkshop IAM roles Workshop custom policies (if any) Optional Resources:\nAWS Security Hub (if not needed) AWS CloudTrail (if not needed) Step 9: Cleanup Verification Check Remaining Resources Check AWS Cost Explorer to confirm no charges are ongoing Use AWS Resource Groups to find tagged resources Search for tag: Project=IdentityGovernance Check Final Services AWS Config: Disable configuration recorder if not needed AWS Security Hub: Disable if not used elsewhere Amazon GuardDuty: Disable if not needed AWS Audit Manager: Disable data collection Workshop Cleanup Completed Workshop cleanup completed successfully! 🎉\n"
},
{
	"uri": "//localhost:62814/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:62814/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]